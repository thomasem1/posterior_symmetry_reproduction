{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from torch_uncertainty import TUTrainer\n",
    "from torch_uncertainty.datamodules import MNISTDataModule\n",
    "from torch_uncertainty.losses import ELBOLoss\n",
    "from torch_uncertainty.models.lenet import bayesian_lenet\n",
    "from torch_uncertainty.models import mc_dropout\n",
    "from torch_uncertainty.routines import ClassificationRoutine\n",
    "\n",
    "from laplace import Laplace\n",
    "\n",
    "from pathlib import Path\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OptuNet Posterior Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_PATH = \"data\"\n",
    "\n",
    "# Parameters from paper\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.04\n",
    "WEIGHT_DECAY = 2e-4\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "## OptuNet params\n",
    "DROPOUT_RATE = 0.2 # last layer dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "root = Path(DATA_PATH)\n",
    "datamodule = MNISTDataModule(root=root, batch_size=BATCH_SIZE, eval_ood=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OptuNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptuNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Add layers for OptuNet (use Section C.2.1 from the paper for details)\n",
    "        # Layers: Conv2D (out_ch=2, ks=4, groups=1) -> Max Pooling (ks=3, stride=3) -> ReLU -> Conv2D (out_ch=10, ks=5, groups=2) -> Average Pooling -> ReLU -> Linear 10x10\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=4, groups=1, bias=False)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=10, kernel_size=5, groups=2, bias=False)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.pool1(self.conv1(x)))  # First conv, max pooling, ReLU\n",
    "        x = self.relu(self.pool2(self.conv2(x)))  # Second conv, avg pooling, ReLU\n",
    "        x = self.fc1(x)  # Linear layer\n",
    "        return x\n",
    "\n",
    "def load_optunet_model(version: int):\n",
    "    \"\"\"Load the model corresponding to the given version.\"\"\"\n",
    "    model = OptuNet(num_classes=datamodule.num_classes)\n",
    "    path = Path(f\"models/mnist-optunet-0-8191/version_{version}.safetensors\")\n",
    "\n",
    "    if not path.exists():\n",
    "        raise ValueError(\"File does not exist\")\n",
    "\n",
    "    state_dict = load_file(path)\n",
    "\n",
    "    model.load_state_dict(state_dict=state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_lenet(model: nn.Module):\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=0.04,\n",
    "        weight_decay=0.0002\n",
    "    )\n",
    "    return optimizer\n",
    "\n",
    "trainer = TUTrainer(accelerator=\"gpu\", enable_progress_bar=False, max_epochs=1)\n",
    "\n",
    "# model\n",
    "# model = bayesian_lenet(datamodule.num_channels, datamodule.num_classes)\n",
    "model = load_optunet_model(version=1000)\n",
    "\n",
    "# loss\n",
    "loss = ELBOLoss(\n",
    "    model=model,\n",
    "    inner_loss=nn.CrossEntropyLoss(),\n",
    "    kl_weight=1 / 10000,\n",
    "    num_samples=3,\n",
    ")\n",
    "\n",
    "# learning rate scheduler to  decay\n",
    "#the learning rate twice during training, at epochs 15 and 30, dividing the learning rate by 2.\n",
    "def scheduler_lenet(optimizer):\n",
    "    scheduler = MultiStepLR(\n",
    "        optimizer,\n",
    "        milestones=[15, 30],  # Epochs at which to decay the learning rate\n",
    "        gamma=0.5,            # Factor by which to multiply the learning rate\n",
    "    )\n",
    "    return scheduler\n",
    "\n",
    "routine = ClassificationRoutine(\n",
    "    model=model,\n",
    "    num_classes=datamodule.num_classes,\n",
    "    loss=loss,\n",
    "    optim_recipe=torch.optim.SGD(model.parameters(),lr=0.04,weight_decay=0.0002),\n",
    "    is_ensemble=True\n",
    ")\n",
    "\n",
    "# trainer.fit(model=routine, datamodule=datamodule)\n",
    "results = trainer.test(model=routine, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the training dataloader from the datamodule\n",
    "train_dataloader = datamodule.train_dataloader()\n",
    "# Hessian Laplace approximation\n",
    "# This step approximates the posterior distribution over the model parameters with a Gaussian\n",
    "la = Laplace(\n",
    "    model,                      # The trained model\n",
    "    likelihood='classification', # Specify task type\n",
    "    prior_precision=1.0,         # Regularization term for the prior (hyperparameter)\n",
    "    # subset_of_weights='all',     # Apply Laplace to all model weights\n",
    ")\n",
    "\n",
    "# Fit the Laplace approximation using the training data\n",
    "la.fit(train_dataloader)  \n",
    "\n",
    "# Refine the posterior with the Hessian\n",
    "la.optimize_prior_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels = [], []\n",
    "\n",
    "for batch in datamodule.test_dataloader()[0]:\n",
    "    images, true_labels = batch\n",
    "    with torch.no_grad():\n",
    "        probs = model(images)  # Assuming the model outputs probabilities\n",
    "        predictions.append(probs)\n",
    "        labels.append(true_labels)\n",
    "\n",
    "predictions = torch.cat(predictions).numpy()\n",
    "labels = torch.cat(labels).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUPR score\n",
    "from sklearn.metrics import average_precision_score, auc, roc_curve, precision_recall_curve\n",
    "\n",
    "# Get predicted by index based on the highest probability\n",
    "ypreds = predictions.argmax(axis=1)\n",
    "\n",
    "print(ypreds.shape)\n",
    "\n",
    "\n",
    "# Compute precision-recall curve\n",
    "precision, recall = precision_recall_curve(labels, ypreds)\n",
    "aupr = auc(recall, precision)\n",
    "\n",
    "print(f\"AUPR: {aupr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUPR\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(labels_np, positive_probs)\n",
    "aupr = auc(recall, precision)\n",
    "print(f\"AUPR: {aupr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPR95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(labels_np, positive_probs)\n",
    "# Find the threshold where TPR is closest to 0.95\n",
    "threshold_at_95_tpr = thresholds[np.argmax(tpr >= 0.95)]\n",
    "fpr95 = fpr[np.argmax(tpr >= 0.95)]\n",
    "print(f\"FPR95: {fpr95}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# For multi-class classification\n",
    "predictions = np.argmax(probs_np, axis=1)\n",
    "\n",
    "# For binary classification (based on a 0.5 threshold)\n",
    "predictions = (positive_probs >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(labels_np, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.swa_gaussian.swag.posteriors import SWAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
