{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior Symmetry Reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR, LambdaLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_uncertainty import TUTrainer\n",
    "from torch_uncertainty.datamodules import MNISTDataModule\n",
    "from torch_uncertainty.losses import ELBOLoss\n",
    "from torch_uncertainty.models.lenet import bayesian_lenet, lenet\n",
    "from torch_uncertainty.models import mc_dropout\n",
    "from torch_uncertainty.routines import ClassificationRoutine\n",
    "from lightning.pytorch import LightningModule\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "from laplace import Laplace\n",
    "from utils.swa_gaussian.swag.posteriors import SWAG\n",
    "import utils.swa_gaussian.swag.posteriors as swag_posteriors\n",
    "from utils.posterior_symmetry.mmd.mmd_torch import mmdagg\n",
    "from utils.posterior_symmetry.symmetries.permutation import Permuter\n",
    "from utils.posterior_symmetry.symmetries.scale import Scaler\n",
    "from utils.bayes_neural_networks.src.Stochastic_Gradient_HMC_SA.optimizers import H_SA_SGHMC\n",
    "\n",
    "from pathlib import Path\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_PATH = \"data\"\n",
    "MODEL_PATH = Path(\"models\", \"trained_optunets\")\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Parameters from paper\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.04\n",
    "WEIGHT_DECAY = 2e-4\n",
    "\n",
    "## Method params\n",
    "DROPOUT_RATE = 0.2 # last layer dropout rate\n",
    "\n",
    "# Models\n",
    "NMODELS = 100\n",
    "ENSEMBLE_MODELS = 10 # number of models to use in ensemble\n",
    "N_SAMPLES = 100 # number of samples to draw from model\n",
    "\n",
    "MC_SAMPLES = 3 # Posterior samples for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "root = Path(DATA_PATH)\n",
    "datamodule = MNISTDataModule(root=root, batch_size=BATCH_SIZE, eval_ood=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OptuNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptuNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Add layers for OptuNet (use Section C.2.1 from the paper for details)\n",
    "        # Layers: Conv2D (out_ch=2, ks=4, groups=1) -> Max Pooling (ks=3, stride=3) -> ReLU -> Conv2D (out_ch=10, ks=5, groups=2) -> Average Pooling -> ReLU -> Linear 10x10\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=4, groups=1, bias=False)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=10, kernel_size=5, groups=2, bias=False)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.pool1(self.conv1(x)))  # First conv, max pooling, ReLU\n",
    "        x = self.relu(self.pool2(self.conv2(x)))  # Second conv, avg pooling, ReLU\n",
    "        x = torch.mean(x, dim=(2, 3))\n",
    "        x = self.fc1(x)  # Linear layer\n",
    "        return x\n",
    "\n",
    "# Optimizer and LR scheduler\n",
    "def optim_optunet(model: nn.Module):\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer,\n",
    "        milestones=[15, 30],\n",
    "        gamma=0.5\n",
    "    )\n",
    "    return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "# Loss function\n",
    "def loss_optunet(model: nn.Module):\n",
    "    loss = ELBOLoss(\n",
    "        model=model,\n",
    "        inner_loss=nn.CrossEntropyLoss(),\n",
    "        kl_weight= 1/10000,\n",
    "        num_samples=3,\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions\n",
    "def load_optunet_model(version: int):\n",
    "    model = OptuNet()\n",
    "    path = Path(f\"models/mnist-optunet-0-8191/version_{version}.safetensors\")\n",
    "\n",
    "    if not path.exists():\n",
    "        raise ValueError(\"File does not exist\")\n",
    "\n",
    "    state_dict = load_file(path)\n",
    "    model.load_state_dict(state_dict=state_dict)\n",
    "    return model\n",
    "\n",
    "def load_trained_optunet(path):\n",
    "    checkpoint = torch.load(path)\n",
    "\n",
    "    # Filter out unwanted keys (e.g., those related to loss)\n",
    "    state_dict = {\n",
    "        k.replace(\"model.\", \"\"): v\n",
    "        for k, v in checkpoint[\"state_dict\"].items()\n",
    "        if not k.startswith(\"loss.\")\n",
    "    }\n",
    "    model = OptuNet()\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_optunets(n_models = 100, start_idx = 0, tag=\"\"):\n",
    "    # tag (str) is used to tag save name with specific tag\n",
    "\n",
    "    # Train n_models OptuNets\n",
    "    for i in range(start_idx, n_models):\n",
    "        model = OptuNet()\n",
    "\n",
    "        trainer = TUTrainer(\n",
    "            accelerator=\"gpu\",\n",
    "            enable_progress_bar=False,\n",
    "            max_epochs=EPOCHS)\n",
    "        \n",
    "        # loss\n",
    "        loss = ELBOLoss(\n",
    "            model=model,\n",
    "            inner_loss=nn.CrossEntropyLoss(),\n",
    "            kl_weight=1/10000,\n",
    "            num_samples=3,\n",
    "        )\n",
    "\n",
    "        routine = ClassificationRoutine(\n",
    "            model=model,\n",
    "            num_classes=datamodule.num_classes,\n",
    "            loss=loss,\n",
    "            optim_recipe=optim_optunet(model),\n",
    "            is_ensemble=True\n",
    "        )\n",
    "\n",
    "        trainer.fit(model=routine, datamodule=datamodule)\n",
    "\n",
    "        # Save the trained model\n",
    "        save_path = Path(MODEL_PATH, f\"model_{tag}{i}.pt\")\n",
    "        trainer.save_checkpoint(save_path)\n",
    "    \n",
    "    print(f\"Trained {n_models} models. Saved to {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_optunets(n_models=NMODELS, start_idx=30, tag=\"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_cosine_scheduler(model, warmup_steps, total_steps, min_lr=0, max_lr=0.04):\n",
    "    # Define the learning rate scheduler as a Lambda function\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_steps:\n",
    "            # Linear warmup: Increase from 0 to max_lr\n",
    "            return float(epoch) / float(max(1, warmup_steps))\n",
    "        else:\n",
    "            # Cosine decay after warmup\n",
    "            progress = (epoch - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "            return min_lr + 0.5 * (max_lr - min_lr) * (1 + torch.cos(torch.pi * progress))\n",
    "\n",
    "    optimizer = optim_optunet(model)[\"optimizer\"]\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(model, path):\n",
    "    # Ensure path exists\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_models(n_models = 100, start_idx = 0, tag=\"\"):\n",
    "    posterior_models = []\n",
    "\n",
    "    for i in range(start_idx, n_models):\n",
    "        path = Path(MODEL_PATH, f\"model_{tag}{i}.pt\")\n",
    "        model = load_trained_optunet(path)\n",
    "        model = model.to(DEVICE) # Needed?\n",
    "        posterior_models.append(model)\n",
    "    \n",
    "    print(f\"Loaded {len(posterior_models)} models\")\n",
    "    return posterior_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 31 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_2164\\3394468100.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "posterior_models = load_trained_models(n_models=31, tag=\"t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptuDrop(OptuNet):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(super().forward(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model            | OptuDrop         | 392    | train\n",
      "1 | loss             | CrossEntropyLoss | 0      | train\n",
      "2 | format_batch_fn  | Identity         | 0      | train\n",
      "3 | val_cls_metrics  | MetricCollection | 0      | train\n",
      "4 | test_cls_metrics | MetricCollection | 0      | train\n",
      "5 | test_id_entropy  | Entropy          | 0      | train\n",
      "6 | mixup            | Identity         | 0      | train\n",
      "--------------------------------------------------------------\n",
      "392       Trainable params\n",
      "0         Non-trainable params\n",
      "392       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "32        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:09<00:00, 94.13it/s, v_num=106, train_loss=1.910]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 938/938 [00:05<00:00, 163.68it/s, v_num=106, train_loss=1.650, Acc%=41.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 938/938 [00:06<00:00, 154.54it/s, v_num=106, train_loss=1.680, Acc%=48.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 938/938 [00:06<00:00, 147.46it/s, v_num=106, train_loss=1.620, Acc%=65.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 938/938 [00:05<00:00, 157.83it/s, v_num=106, train_loss=1.710, Acc%=67.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 938/938 [00:05<00:00, 164.73it/s, v_num=106, train_loss=1.250, Acc%=65.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 938/938 [00:05<00:00, 158.81it/s, v_num=106, train_loss=1.640, Acc%=71.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 938/938 [00:05<00:00, 160.11it/s, v_num=106, train_loss=1.310, Acc%=68.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 938/938 [00:06<00:00, 151.05it/s, v_num=106, train_loss=1.070, Acc%=71.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 938/938 [00:06<00:00, 143.31it/s, v_num=106, train_loss=1.230, Acc%=74.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 938/938 [00:05<00:00, 159.40it/s, v_num=106, train_loss=1.050, Acc%=72.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 938/938 [00:05<00:00, 162.21it/s, v_num=106, train_loss=1.260, Acc%=76.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 938/938 [00:06<00:00, 144.78it/s, v_num=106, train_loss=1.490, Acc%=68.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 938/938 [00:06<00:00, 142.87it/s, v_num=106, train_loss=0.844, Acc%=75.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 938/938 [00:06<00:00, 151.33it/s, v_num=106, train_loss=0.933, Acc%=74.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 938/938 [00:06<00:00, 153.40it/s, v_num=106, train_loss=1.250, Acc%=73.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 938/938 [00:06<00:00, 148.41it/s, v_num=106, train_loss=1.200, Acc%=77.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 938/938 [00:06<00:00, 144.26it/s, v_num=106, train_loss=1.040, Acc%=76.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 938/938 [00:06<00:00, 150.53it/s, v_num=106, train_loss=1.030, Acc%=77.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 938/938 [00:06<00:00, 145.90it/s, v_num=106, train_loss=0.948, Acc%=76.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 938/938 [00:06<00:00, 154.13it/s, v_num=106, train_loss=1.170, Acc%=79.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 938/938 [00:06<00:00, 142.56it/s, v_num=106, train_loss=1.210, Acc%=78.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 938/938 [00:07<00:00, 128.11it/s, v_num=106, train_loss=1.190, Acc%=83.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 938/938 [00:07<00:00, 127.97it/s, v_num=106, train_loss=1.190, Acc%=83.20]\n"
     ]
    }
   ],
   "source": [
    "model = OptuDrop()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "routine = ClassificationRoutine(\n",
    "    model=model,\n",
    "    num_classes=datamodule.num_classes,\n",
    "    loss=loss_fn,\n",
    "    optim_recipe=optim_optunet(model),\n",
    "    is_ensemble=False\n",
    ")\n",
    "\n",
    "trainer = TUTrainer(\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    max_epochs=EPOCHS\n",
    ")\n",
    "\n",
    "trainer.fit(model=routine, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:01<00:00, 88.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">      Classification       </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     Acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          83.21%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    Brier     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.26384          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   Entropy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.81296          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     NLL      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.56206          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Calibration        </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     ECE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.10622          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     aECE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.10622          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\"> Selective Classification  </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    AUGRC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           3.50%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     AURC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           4.51%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Cov@5Risk   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          65.15%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Risk@80Cov  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           8.86%           </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Classification      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    Acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         83.21%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   Brier    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.26384         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  Entropy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.81296         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    NLL     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.56206         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Calibration       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    ECE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.10622         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    aECE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.10622         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSelective Classification \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   AUGRC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          3.50%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    AURC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          4.51%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Cov@5Risk  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         65.15%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Risk@80Cov \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          8.86%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing\n",
    "results = trainer.test(model=routine, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viBNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model            | OptuNet          | 392    | train\n",
      "1 | loss             | CrossEntropyLoss | 0      | train\n",
      "2 | format_batch_fn  | Identity         | 0      | train\n",
      "3 | val_cls_metrics  | MetricCollection | 0      | train\n",
      "4 | test_cls_metrics | MetricCollection | 0      | train\n",
      "5 | test_id_entropy  | Entropy          | 0      | train\n",
      "6 | mixup            | Identity         | 0      | train\n",
      "--------------------------------------------------------------\n",
      "392       Trainable params\n",
      "0         Non-trainable params\n",
      "392       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:10<00:00, 92.33it/s, v_num=107, train_loss=1.170]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 938/938 [00:05<00:00, 159.95it/s, v_num=107, train_loss=0.771, Acc%=65.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 938/938 [00:05<00:00, 157.90it/s, v_num=107, train_loss=0.803, Acc%=75.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 938/938 [00:06<00:00, 140.95it/s, v_num=107, train_loss=0.795, Acc%=69.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 938/938 [00:06<00:00, 139.49it/s, v_num=107, train_loss=0.936, Acc%=79.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 938/938 [00:06<00:00, 141.74it/s, v_num=107, train_loss=0.894, Acc%=65.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 938/938 [00:07<00:00, 123.50it/s, v_num=107, train_loss=0.911, Acc%=67.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 938/938 [00:06<00:00, 139.40it/s, v_num=107, train_loss=0.701, Acc%=78.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 938/938 [00:07<00:00, 130.27it/s, v_num=107, train_loss=1.060, Acc%=83.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 938/938 [00:06<00:00, 143.14it/s, v_num=107, train_loss=0.676, Acc%=80.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:06<00:00, 140.80it/s, v_num=107, train_loss=0.657, Acc%=75.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 938/938 [00:05<00:00, 174.33it/s, v_num=107, train_loss=1.100, Acc%=84.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 938/938 [00:05<00:00, 162.57it/s, v_num=107, train_loss=1.050, Acc%=83.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119: 100%|██████████| 938/938 [00:06<00:00, 140.39it/s, v_num=107, train_loss=0.606, Acc%=85.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=120` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119: 100%|██████████| 938/938 [00:06<00:00, 140.26it/s, v_num=107, train_loss=0.606, Acc%=85.30]\n"
     ]
    }
   ],
   "source": [
    "# Train OptuNet for SWAG\n",
    "model = OptuNet()\n",
    "\n",
    "routine = ClassificationRoutine(\n",
    "    model=model,\n",
    "    num_classes=datamodule.num_classes,\n",
    "    loss=nn.CrossEntropyLoss(),\n",
    "    optim_recipe=optim_optunet(model),\n",
    "    is_ensemble=False # Single model (not ensemble here)\n",
    ")\n",
    "\n",
    "trainer = TUTrainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=2*EPOCHS, # Train twice as long for SWAG\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(model=routine, datamodule=datamodule)\n",
    "\n",
    "# Save checkpoints every 10 epochs from epoch 80 onward\n",
    "checkpoint_dir = Path(\"models/swag_checkpoints/\")\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "checkpoints = []\n",
    "for epoch in range(80, 121, 10):\n",
    "    checkpoint_path = checkpoint_dir / f\"model_epoch_{epoch}.pt\"\n",
    "    trainer.save_checkpoint(checkpoint_path)\n",
    "    checkpoints.append(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_2164\\2627075174.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_2164\\3394468100.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "# Create the SWAG object\n",
    "swag_model = SWAG(\n",
    "    base=OptuNet,\n",
    "    max_num_models=20,\n",
    "    var_clamp=1e-30\n",
    ")\n",
    "\n",
    "# Add the collected checkpoints to the SWAG posterior\n",
    "for checkpoint_path in checkpoints:\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    trained_model = load_trained_optunet(checkpoint_path)\n",
    "    swag_model.collect_model(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_uncertainty.models import SWAG as SWAGModel\n",
    "\n",
    "# Load the SWAG model\n",
    "swag_model = SWAGModel(model=model, cycle_start=80, cycle_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWAGLightningWrapper(LightningModule):\n",
    "    def __init__(self, swag_model, num_samples=10, scale=0.1):\n",
    "        super().__init__()\n",
    "        self.swag_model = swag_model\n",
    "        self.num_samples = num_samples\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        preds = []\n",
    "        self.swag_model.to(DEVICE)\n",
    "        for _ in range(self.num_samples):\n",
    "            sampled_model = self.swag_model.sample(scale=self.scale)  # Sample from SWAG posterior\n",
    "            sampled_model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds.append(sampled_model(x))\n",
    "        return torch.stack(preds).mean(dim=0)  # Aggregate predictions\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        return self.forward(x)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Test step for evaluating the model on the test set.\n",
    "        Args:\n",
    "            batch: A batch of test data.\n",
    "            batch_idx: The index of the batch.\n",
    "        Returns:\n",
    "            Dict with predictions and optionally other metrics.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        preds = self.forward(x)\n",
    "        loss = nn.CrossEntropyLoss()(preds, y)\n",
    "        acc = (preds.argmax(dim=1) == y).float().mean()\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "        return {\"test_loss\": loss, \"test_acc\": acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: |          | 0/? [00:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[217], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m swag_wrapper \u001b[38;5;241m=\u001b[39m swag_wrapper\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Test the SWAG model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswag_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSWAG Test Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:748\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:788\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    785\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    787\u001b[0m )\n\u001b[1;32m--> 788\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[0;32m    790\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1018\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[1;32m-> 1018\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    395\u001b[0m )\n\u001b[1;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:424\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[216], line 32\u001b[0m, in \u001b[0;36mSWAGLightningWrapper.test_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03mTest step for evaluating the model on the test set.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    Dict with predictions and optionally other metrics.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 32\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(preds, y)\n\u001b[0;32m     34\u001b[0m acc \u001b[38;5;241m=\u001b[39m (preds\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[1;32mIn[216], line 15\u001b[0m, in \u001b[0;36mSWAGLightningWrapper.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m     sampled_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 15\u001b[0m         preds\u001b[38;5;241m.\u001b[39mappend(\u001b[43msampled_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(preds)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mOptuNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))  \u001b[38;5;66;03m# First conv, max pooling, ReLU\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))  \u001b[38;5;66;03m# Second conv, avg pooling, ReLU\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(x, dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# Wrap SWAG model for testing\n",
    "swag_wrapper = SWAGLightningWrapper(\n",
    "    swag_model=swag_model.to(DEVICE),\n",
    "    num_samples=10,  # Number of posterior samples as per the paper\n",
    "    scale=0.1        # Scale parameter for SWAG sampling\n",
    ")\n",
    "swag_wrapper = swag_wrapper.to(DEVICE)\n",
    "\n",
    "# Test the SWAG model\n",
    "results = trainer.test(model=swag_wrapper, datamodule=datamodule)\n",
    "print(\"SWAG Test Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: |          | 0/? [10:26<?, ?it/s]\n",
      "Testing: |          | 157/? [00:06<00:00, 23.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">      Classification       </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     Acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          85.34%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    Brier     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.21737          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   Entropy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.50600          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     NLL      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.45356          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Calibration        </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     ECE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.02825          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     aECE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.02804          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\"> Selective Classification  </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    AUGRC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           2.83%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     AURC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           3.56%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Cov@5Risk   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          72.81%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Risk@80Cov  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           7.01%           </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Classification      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    Acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         85.34%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   Brier    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.21737         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  Entropy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.50600         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    NLL     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.45356         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Calibration       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    ECE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.02825         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    aECE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.02804         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSelective Classification \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   AUGRC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          2.83%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    AURC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          3.56%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Cov@5Risk  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         72.81%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Risk@80Cov \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          7.01%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/cal/ECE': 0.028251025825738907,\n",
       "  'test/cal/aECE': 0.028039993718266487,\n",
       "  'test/cls/Acc': 0.8533999919891357,\n",
       "  'test/cls/Brier': 0.2173655778169632,\n",
       "  'test/cls/NLL': 0.45356234908103943,\n",
       "  'test/sc/AUGRC': 0.028299571946263313,\n",
       "  'test/sc/AURC': 0.03561488911509514,\n",
       "  'test/sc/Cov@5Risk': 0.7281000018119812,\n",
       "  'test/sc/Risk@80Cov': 0.07012499868869781,\n",
       "  'test/cls/Entropy': 0.50600266456604}]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=routine, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model            | OptuNet          | 392    | train\n",
      "1 | loss             | CrossEntropyLoss | 0      | train\n",
      "2 | format_batch_fn  | Identity         | 0      | train\n",
      "3 | val_cls_metrics  | MetricCollection | 0      | train\n",
      "4 | test_cls_metrics | MetricCollection | 0      | train\n",
      "5 | test_id_entropy  | Entropy          | 0      | train\n",
      "6 | mixup            | Identity         | 0      | train\n",
      "--------------------------------------------------------------\n",
      "392       Trainable params\n",
      "0         Non-trainable params\n",
      "392       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:09<00:00, 96.12it/s, v_num=109, train_loss=1.340]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 938/938 [00:06<00:00, 138.13it/s, v_num=109, train_loss=0.899, Acc%=60.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 938/938 [00:07<00:00, 118.59it/s, v_num=109, train_loss=1.210, Acc%=69.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 938/938 [00:08<00:00, 110.80it/s, v_num=109, train_loss=0.817, Acc%=66.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 938/938 [00:06<00:00, 152.62it/s, v_num=109, train_loss=0.936, Acc%=69.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 938/938 [00:06<00:00, 146.54it/s, v_num=109, train_loss=0.989, Acc%=73.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 938/938 [00:06<00:00, 149.86it/s, v_num=109, train_loss=0.871, Acc%=65.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 938/938 [00:06<00:00, 151.31it/s, v_num=109, train_loss=1.160, Acc%=74.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 938/938 [00:06<00:00, 138.71it/s, v_num=109, train_loss=0.910, Acc%=74.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 938/938 [00:06<00:00, 142.82it/s, v_num=109, train_loss=1.160, Acc%=72.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 938/938 [00:07<00:00, 123.83it/s, v_num=109, train_loss=1.160, Acc%=70.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 938/938 [00:07<00:00, 123.72it/s, v_num=109, train_loss=1.160, Acc%=70.20]\n"
     ]
    }
   ],
   "source": [
    "from torch_uncertainty.models import SWAG\n",
    "from torch_uncertainty.models import SWA\n",
    "\n",
    "# Instantiate the model\n",
    "model = OptuNet()\n",
    "\n",
    "# Define SWAG model, using the number of samples to approximate the posterior\n",
    "swag_model = SWAG(model, cycle_start=2, cycle_length=5)  # Set the number of samples for SWAG\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Set up the ClassificationRoutine\n",
    "routine = ClassificationRoutine(model,\n",
    "                                num_classes=10,\n",
    "                                loss=nn.CrossEntropyLoss(), \n",
    "                                optim_recipe=optim_optunet(model),\n",
    "                                is_ensemble=False)\n",
    "\n",
    "# Instantiate the trainer with the routine\n",
    "trainer = TUTrainer(accelerator=\"gpu\", max_epochs=10, enable_progress_bar=True)\n",
    "\n",
    "# Start the training process\n",
    "trainer.fit(model=routine, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 70.22%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in datamodule.test_dataloader()[0]:\n",
    "        output = swag_model(data)\n",
    "        preds = output.argmax(dim=1)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(target)\n",
    "\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "accuracy = accuracy_score(all_labels.numpy(), all_preds.numpy())\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SWAG.__init__() missing 2 required positional arguments: 'cycle_start' and 'cycle_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[221], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize SWAG for posterior estimation\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m swag_model \u001b[38;5;241m=\u001b[39m \u001b[43mSWAGWithUncertainty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOptuNet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Training loop (example, you would typically use a trainer like TUTrainer or LightningTrainer)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "Cell \u001b[1;32mIn[220], line 18\u001b[0m, in \u001b[0;36mSWAGWithUncertainty.__init__\u001b[1;34m(self, base_model, max_num_models, var_clamp)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_clamp \u001b[38;5;241m=\u001b[39m var_clamp\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Store models\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswag \u001b[38;5;241m=\u001b[39m \u001b[43mTU_Swag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_num_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_clamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar_clamp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: SWAG.__init__() missing 2 required positional arguments: 'cycle_start' and 'cycle_length'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Initialize the model and trainer\n",
    "model = OptuNet()  # Example model\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Initialize SWAG for posterior estimation\n",
    "swag_model = SWAGWithUncertainty(base_model=OptuNet, max_num_models=20)\n",
    "\n",
    "# Training loop (example, you would typically use a trainer like TUTrainer or LightningTrainer)\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Train the model as usual\n",
    "    model.train()\n",
    "    for batch in DataLoader(datamodule.train_data):\n",
    "        x, y = batch\n",
    "        output = model(x)\n",
    "        loss = nn.CrossEntropyLoss()(output, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # After every few epochs, collect the model for SWAG\n",
    "    if epoch >= 80 and epoch % 10 == 0:\n",
    "        swag_model.collect_model(model)\n",
    "\n",
    "# After training, you can test the SWAG model\n",
    "sampled_model = swag_model.sample(scale=0.1)  # Sample from the posterior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model            | OptuNet          | 392    | train\n",
      "1 | loss             | CrossEntropyLoss | 0      | train\n",
      "2 | format_batch_fn  | Identity         | 0      | train\n",
      "3 | val_cls_metrics  | MetricCollection | 0      | train\n",
      "4 | test_cls_metrics | MetricCollection | 0      | train\n",
      "5 | test_id_entropy  | Entropy          | 0      | train\n",
      "6 | mixup            | Identity         | 0      | train\n",
      "--------------------------------------------------------------\n",
      "392       Trainable params\n",
      "0         Non-trainable params\n",
      "392       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:09<00:00, 96.90it/s, v_num=57, train_loss=1.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 938/938 [00:05<00:00, 160.81it/s, v_num=57, train_loss=1.070, Acc%=52.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 938/938 [00:06<00:00, 143.72it/s, v_num=57, train_loss=1.740, Acc%=59.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 938/938 [00:06<00:00, 139.31it/s, v_num=57, train_loss=0.841, Acc%=53.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 938/938 [00:06<00:00, 144.08it/s, v_num=57, train_loss=1.010, Acc%=73.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 938/938 [00:06<00:00, 148.42it/s, v_num=57, train_loss=1.680, Acc%=65.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 938/938 [00:06<00:00, 148.22it/s, v_num=57, train_loss=0.599, Acc%=76.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 938/938 [00:06<00:00, 143.06it/s, v_num=57, train_loss=0.713, Acc%=79.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 938/938 [00:06<00:00, 138.37it/s, v_num=57, train_loss=1.010, Acc%=79.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 938/938 [00:06<00:00, 144.06it/s, v_num=57, train_loss=0.803, Acc%=80.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 938/938 [00:06<00:00, 144.23it/s, v_num=57, train_loss=0.867, Acc%=74.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 938/938 [00:06<00:00, 151.63it/s, v_num=57, train_loss=1.390, Acc%=80.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:06<00:00, 152.22it/s, v_num=57, train_loss=0.696, Acc%=82.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 938/938 [00:07<00:00, 123.66it/s, v_num=57, train_loss=0.513, Acc%=83.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 938/938 [00:06<00:00, 134.27it/s, v_num=57, train_loss=1.040, Acc%=75.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 938/938 [00:07<00:00, 120.40it/s, v_num=57, train_loss=0.573, Acc%=83.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 938/938 [00:06<00:00, 151.51it/s, v_num=57, train_loss=0.517, Acc%=81.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 938/938 [00:06<00:00, 143.95it/s, v_num=57, train_loss=0.956, Acc%=83.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 938/938 [00:06<00:00, 155.05it/s, v_num=57, train_loss=0.769, Acc%=79.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 938/938 [00:06<00:00, 146.01it/s, v_num=57, train_loss=0.626, Acc%=81.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 938/938 [00:06<00:00, 145.88it/s, v_num=57, train_loss=0.469, Acc%=82.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 938/938 [00:06<00:00, 147.30it/s, v_num=57, train_loss=1.710, Acc%=83.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 938/938 [00:05<00:00, 158.17it/s, v_num=57, train_loss=0.833, Acc%=76.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 938/938 [00:06<00:00, 154.98it/s, v_num=57, train_loss=0.886, Acc%=75.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 938/938 [00:06<00:00, 140.42it/s, v_num=57, train_loss=0.419, Acc%=81.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 938/938 [00:06<00:00, 140.28it/s, v_num=57, train_loss=0.419, Acc%=81.80]\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = OptuNet()\n",
    "\n",
    "def scheduler_laplace(optimizer):\n",
    "    return MultiStepLR(\n",
    "        optimizer,\n",
    "        milestones=[15, 30],\n",
    "        gamma=0.5\n",
    "    )\n",
    "\n",
    "# Routine\n",
    "loss_fn = nn.CrossEntropyLoss()  # Standard cross-entropy loss\n",
    "routine = ClassificationRoutine(\n",
    "    model=model,\n",
    "    num_classes=datamodule.num_classes,\n",
    "    loss=loss_fn,\n",
    "    optim_recipe=optim_optunet(model),\n",
    "    # scheduler_recipe=scheduler_laplace,\n",
    "    is_ensemble=False\n",
    ")\n",
    "\n",
    "# Train the model to MAP estimate\n",
    "trainer = TUTrainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=EPOCHS,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "trainer.fit(model=routine, datamodule=datamodule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\laplace\\baselaplace.py:435: UserWarning: By default `link_approx` is `probit`. Make sure to set it equals to the way you want to call `la(test_data, pred_type=..., link_approx=...)`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Apply Laplace approximation\n",
    "laplace_model = Laplace(model, likelihood='classification', subset_of_weights='last_layer', hessian_structure='full')\n",
    "laplace_model.fit(datamodule.train_dataloader())  # Fit the Laplace model on training data\n",
    "laplace_model.optimize_prior_precision()  # Optimize prior precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:01<00:00, 88.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">      Classification       </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     Acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          81.76%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    Brier     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.27002          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   Entropy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.58099          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     NLL      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.57028          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Calibration        </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     ECE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.02009          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     aECE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.02005          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\"> Selective Classification  </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    AUGRC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           4.41%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     AURC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           5.96%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Cov@5Risk   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          52.26%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Risk@80Cov  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          10.91%           </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Classification      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    Acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         81.76%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   Brier    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.27002         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  Entropy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.58099         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    NLL     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.57028         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Calibration       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    ECE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.02009         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    aECE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.02005         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSelective Classification \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   AUGRC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          4.41%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    AURC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          5.96%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Cov@5Risk  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         52.26%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Risk@80Cov \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         10.91%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace Test Results: [{'test/cal/ECE': 0.020094318315386772, 'test/cal/aECE': 0.02005278505384922, 'test/cls/Acc': 0.8176000118255615, 'test/cls/Brier': 0.2700233459472656, 'test/cls/NLL': 0.5702756643295288, 'test/sc/AUGRC': 0.04408245161175728, 'test/sc/AURC': 0.05958322063088417, 'test/sc/Cov@5Risk': 0.5226000547409058, 'test/sc/Risk@80Cov': 0.10912500321865082, 'test/cls/Entropy': 0.5809915661811829}]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "results = trainer.test(model=routine, datamodule=datamodule)\n",
    "print(\"Laplace Test Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGHMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pSGLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_ensemble(model_class, datamodule, num_ensembles=10, save_dir=\"models/deep_ensemble\"):\n",
    "    \"\"\"\n",
    "    Train a deep ensemble of models.\n",
    "    Args:\n",
    "        model_class: The model class (e.g., OptuNet).\n",
    "        datamodule: Data module providing train/val/test splits.\n",
    "        num_ensembles (int): Number of models in the ensemble.\n",
    "        save_dir (str): Directory to save the trained models.\n",
    "    Returns:\n",
    "        list: Trained models.\n",
    "    \"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    trained_models = []\n",
    "    for i in range(num_ensembles):\n",
    "        print(f\"Training model {i + 1}/{num_ensembles}...\")\n",
    "        \n",
    "        # Initialize a new model\n",
    "        model = model_class()\n",
    "        \n",
    "        # Define loss and optimizer\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        routine = ClassificationRoutine(\n",
    "            model=model,\n",
    "            num_classes=datamodule.num_classes,\n",
    "            loss=loss_fn,\n",
    "            optim_recipe=optim_optunet(model),  # Replace with appropriate optimizer\n",
    "            is_ensemble=False\n",
    "        )\n",
    "\n",
    "        # Trainer\n",
    "        trainer = TUTrainer(\n",
    "            accelerator=\"gpu\",\n",
    "            max_epochs=EPOCHS,\n",
    "            enable_progress_bar=True\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.fit(model=routine, datamodule=datamodule)\n",
    "\n",
    "        # Save the model\n",
    "        model_path = save_path / f\"model_{i+1}.pt\"\n",
    "        trainer.save_checkpoint(model_path)\n",
    "\n",
    "        trained_models.append(model)\n",
    "\n",
    "    return trained_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, dataloader):\n",
    "    \"\"\"\n",
    "    Perform inference using a deep ensemble.\n",
    "    Args:\n",
    "        models (list): List of trained models.\n",
    "        dataloader (DataLoader): DataLoader for test data.\n",
    "    Returns:\n",
    "        np.ndarray: Averaged predictions from the ensemble.\n",
    "    \"\"\"\n",
    "    all_preds = []\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        preds = []\n",
    "\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)  # Logits shape: (batch_size, num_classes)\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "        \n",
    "        all_preds.append(np.concatenate(preds, axis=0))  # Combine batches\n",
    "\n",
    "    # Stack predictions from all models and average\n",
    "    ensemble_preds = np.mean(np.stack(all_preds, axis=0), axis=0)\n",
    "    return ensemble_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_models = train_deep_ensemble(OptuNet, datamodule, num_ensembles=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_confidence_interval(model, dataloader, confidence=0.95):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    lower_bounds = []\n",
    "    upper_bounds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "\n",
    "            # For Monte carlo estimation of the ELBO using 3 samples\n",
    "            preds = torch.stack([model(inputs) for _ in range(MC_SAMPLES)], dim=0)\n",
    "\n",
    "            # Calculate mean and standard deviation\n",
    "            preds_mean = preds.mean(dim=0)\n",
    "            preds_std = preds.std(dim=0)\n",
    "\n",
    "            # Apply softmax to the mean predictions for probabilities\n",
    "            preds_mean = F.softmax(preds_mean, dim=1)\n",
    "\n",
    "\n",
    "            # Compute confidence intervals\n",
    "            z_value = st.norm.ppf(1 - (1 - confidence) / 2)  #dynamically computing the z-score for a given confidence level (e.g., 95%, 99%).\n",
    "            ci_lower = preds_mean - z_value * preds_std\n",
    "            ci_upper = preds_mean + z_value * preds_std\n",
    "\n",
    "            all_preds.append(preds_mean)\n",
    "            all_targets.append(targets)\n",
    "            lower_bounds.append(ci_lower)\n",
    "            upper_bounds.append(ci_upper)\n",
    "\n",
    "    # Concatenate results for all batches\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    lower_bounds = torch.cat(lower_bounds)\n",
    "    upper_bounds = torch.cat(upper_bounds)\n",
    "\n",
    "    return all_preds, all_targets, lower_bounds, upper_bounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_sampling(model, data_loader, num_samples=100):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        sampled_preds = []\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                sampled_preds.append(outputs.cpu().numpy())\n",
    "        predictions.append(np.concatenate(sampled_preds, axis=0))\n",
    "\n",
    "    return np.array(predictions) # Shape: (num_samples, num_examples, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_model_predictions(models, data_loader):\n",
    "    all_predictions = []\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)  # Logits\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "        all_predictions.append(np.concatenate(preds, axis=0))  # Combine batches\n",
    "\n",
    "    return np.array(all_predictions)  # Shape: (num_models, num_datapoints, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_samples(models):\n",
    "    weight_samples = []\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        weights = []\n",
    "        for param in model.parameters():\n",
    "            weights.append(param.detach().cpu().numpy().flatten())  # Flatten weights\n",
    "        weight_samples.append(np.concatenate(weights))\n",
    "\n",
    "    return np.array(weight_samples)  # Shape: (num_models, num_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_source_samples(model, dataloader, num_samples=N_SAMPLES):\n",
    "    model.eval()\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        sampled_weights = []\n",
    "        for param in model.parameters():\n",
    "            sampled_weights.append(param.detach().cpu().numpy().flatten())  # Flatten weights\n",
    "        samples.append(np.concatenate(sampled_weights))\n",
    "\n",
    "    return np.array(samples)  # Shape: (num_samples, num_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mmd(model, posterior_models, test_dataset, num_samples=100):\n",
    "    # Posterior estimation with weights?\n",
    "    target_weights = generate_target_samples(posterior_models)\n",
    "    source_weights = generate_source_samples(model, test_dataset, num_samples=num_samples)\n",
    "\n",
    "    mmd_weights = mmdagg(\n",
    "        X=source_weights,\n",
    "        Y=target_weights,\n",
    "        alpha=0.05,\n",
    "        kernel=\"laplace_gaussian\",\n",
    "        number_bandwidths=10,\n",
    "        weights_type=\"uniform\",\n",
    "        B1=2000,\n",
    "        B2=2000,\n",
    "        B3=50,\n",
    "        seed=42424242\n",
    "    )\n",
    "\n",
    "    # Posterior estimation with predictions?\n",
    "    target_preds = target_model_predictions(posterior_models, test_dataset)\n",
    "    source_preds = monte_carlo_sampling(model, test_dataset, num_samples=num_samples)\n",
    "    target_avg = np.mean(target_preds, axis=1)\n",
    "    source_avg = np.mean(source_preds, axis=1)\n",
    "\n",
    "    mmd_preds = mmdagg(\n",
    "        X=source_avg,\n",
    "        Y=target_avg,\n",
    "        alpha=0.05,\n",
    "        kernel=\"laplace_gaussian\",\n",
    "        number_bandwidths=10,\n",
    "        weights_type=\"uniform\",\n",
    "        B1=2000,\n",
    "        B2=2000,\n",
    "        B3=50,\n",
    "        seed=42424242\n",
    "    )\n",
    "\n",
    "    return mmd_weights, mmd_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMD Weights: [0.87978562 0.82753931 0.76739789 0.70192142 0.6338537  0.56574768\n",
      " 0.49972848 0.43738985 0.37979274 0.32752674 0.98954016 0.95578846\n",
      " 0.88053464 0.7639361  0.62464093 0.485872   0.36376301 0.26491255\n",
      " 0.18921433 0.13334578]\n",
      "MMD Predictions: [1.08715523 1.09636101 1.07844432 1.02722529 0.94505782 0.84081791\n",
      " 0.72590885 0.61077981 0.50307482 0.40727815 1.04672222 1.10627945\n",
      " 1.17772252 1.20458561 1.12528148 0.93853321 0.70546051 0.48970462\n",
      " 0.32178229 0.20403273]\n"
     ]
    }
   ],
   "source": [
    "mmd_weights, mmd_preds = calculate_mmd(model, posterior_models, datamodule.test_dataloader()[0], num_samples=N_SAMPLES)\n",
    "print(f\"MMD Weights: {mmd_weights}\\nAverage: {np.sum(mmd_weights)}\")\n",
    "print(\"*\" * 50)\n",
    "print(f\"MMD Predictions: {mmd_preds}\\nAverage: {np.sum(mmd_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aupr_score(predictions, labels):\n",
    "    n_classes = predictions.shape[1]\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    aupr = {}\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        # Binarize the labels for class i\n",
    "        binary_labels = (labels == i).astype(int)\n",
    "        precision[i], recall[i], _ = precision_recall_curve(binary_labels, predictions[:, i])\n",
    "        aupr[i] = auc(recall[i], precision[i])\n",
    "\n",
    "    # Optional: Aggregate AUPR\n",
    "    mean_aupr = np.mean(list(aupr.values()))\n",
    "    return mean_aupr, aupr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPR95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpr95_score(predictions, labels):\n",
    "    # Calculate predicted classes and confidences\n",
    "    predicted_classes = np.argmax(predictions, axis=1)  # Class with highest probability\n",
    "    confidences = np.max(predictions, axis=1)           # Confidence scores (max probability)\n",
    "    binary_labels = (predicted_classes == labels).astype(int) # Determine binary labels (1 for correct, 0 for incorrect)\n",
    "    fpr, tpr, thresholds = roc_curve(binary_labels, confidences)\n",
    "\n",
    "    # Find the threshold where TPR is closest to 95%\n",
    "    idx = np.where(tpr >= 0.95)[0][0]\n",
    "    return fpr[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#considering all classes:\n",
    "def ace_score(predictions, labels, n_bins=10):\n",
    "    # Calculate predicted probabilities and true labels\n",
    "    predicted_probs = predictions.cpu().numpy()\n",
    "    true_labels = labels.cpu().numpy()\n",
    "\n",
    "    # Binarize true labels for calibration\n",
    "    true_labels = (true_labels == np.arange(predicted_probs.shape[1]))  # Assuming multi-class\n",
    "\n",
    "    # Compute the calibration curve\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(true_labels, predicted_probs, n_bins=n_bins)\n",
    "\n",
    "    # Calculate ACE as the mean absolute difference\n",
    "    ace = np.mean(np.abs(fraction_of_positives - mean_predicted_value))\n",
    "    print(f\"ace score: {ace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ace(preds, targets, num_bins=10):\n",
    "    # Convert preds to probabilities\n",
    "    probs = torch.softmax(preds, dim=1).cpu().numpy()  # For multi-class, probs[:, 1] for class 1, or probs[:, positive_class] for specific class\n",
    "    true_targets = targets.cpu().numpy()\n",
    "    positive_class = 1\n",
    "\n",
    "    # For binary classification, you can use probs[:, 1] for class 1 (positive_class)\n",
    "    prob_true = probs[:, positive_class]\n",
    "\n",
    "    binary_targets = (true_targets == positive_class).astype(int)\n",
    "\n",
    "    # Get calibration curve: This will return the true fraction of positives and predicted probabilities for each bin\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(binary_targets, prob_true, n_bins=num_bins)\n",
    "\n",
    "    # Calculate ACE: This is the average absolute difference between the true fraction of positives and the predicted probability\n",
    "    ace = np.mean(np.abs(fraction_of_positives - mean_predicted_value))\n",
    "    return ace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, posterior_models, test_loader):\n",
    "    model.to(DEVICE)\n",
    "    preds, targets, lower_bounds, upper_bounds = evaluate_confidence_interval(model, test_loader)\n",
    "    predictions = preds.cpu().numpy()\n",
    "    labels = targets.cpu().numpy()\n",
    "\n",
    "    # Scores\n",
    "    mean_aupr, aupr = aupr_score(predictions, labels)\n",
    "    fpr95 = fpr95_score(predictions, labels)\n",
    "    ace = calculate_ace(preds, targets)\n",
    "    mmd_weights, mmd_preds = calculate_mmd(model, posterior_models, test_loader, num_samples=N_SAMPLES)\n",
    "\n",
    "    print(f\"AUPR: {mean_aupr}\")\n",
    "    print(f\"FPR95: {fpr95}\")\n",
    "    print(f\"ACE: {ace:.4f}\")\n",
    "    print(f\"MMD Weights: {np.sum(mmd_weights)}\")\n",
    "    print(\"*\" * 50)\n",
    "    print(f\"MMD Predictions: {np.sum(mmd_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPR: 0.795600907203929\n",
      "FPR95: 0.7253190060443251\n",
      "ACE: 0.3760\n",
      "MMD Weights: 11.402368737586935\n",
      "**************************************************\n",
      "MMD Predictions: 14.9937200759505\n"
     ]
    }
   ],
   "source": [
    "score_model(model, posterior_models, datamodule.test_dataloader()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
