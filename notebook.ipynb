{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior Symmetry Reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from torch_uncertainty import TUTrainer\n",
    "from torch_uncertainty.datamodules import MNISTDataModule\n",
    "from torch_uncertainty.losses import ELBOLoss\n",
    "from torch_uncertainty.models.lenet import bayesian_lenet, lenet\n",
    "from torch_uncertainty.models import mc_dropout\n",
    "from torch_uncertainty.routines import ClassificationRoutine\n",
    "from lightning.pytorch import LightningModule\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "from laplace import Laplace\n",
    "from utils.swa_gaussian.swag.posteriors import SWAG\n",
    "import utils.swa_gaussian.swag.posteriors as swag_posteriors\n",
    "from utils.posterior_symmetry.mmd.mmd_torch import mmdagg\n",
    "\n",
    "from pathlib import Path\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_PATH = \"data\"\n",
    "\n",
    "# Parameters from paper\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.04\n",
    "WEIGHT_DECAY = 2e-4\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "## OptuNet params\n",
    "DROPOUT_RATE = 0.2 # last layer dropout rate\n",
    "\n",
    "# Models\n",
    "NMODELS = 100\n",
    "MODEL_PATH = Path(\"models\", \"trained_optunets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "root = Path(DATA_PATH)\n",
    "datamodule = MNISTDataModule(root=root, batch_size=BATCH_SIZE, eval_ood=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OptuNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptuNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Add layers for OptuNet (use Section C.2.1 from the paper for details)\n",
    "        # Layers: Conv2D (out_ch=2, ks=4, groups=1) -> Max Pooling (ks=3, stride=3) -> ReLU -> Conv2D (out_ch=10, ks=5, groups=2) -> Average Pooling -> ReLU -> Linear 10x10\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=4, groups=1, bias=False)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=10, kernel_size=5, groups=2, bias=False)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.pool1(self.conv1(x)))  # First conv, max pooling, ReLU\n",
    "        x = self.relu(self.pool2(self.conv2(x)))  # Second conv, avg pooling, ReLU\n",
    "        x = torch.mean(x, dim=(2, 3))\n",
    "        x = self.fc1(x)  # Linear layer\n",
    "        return x\n",
    "\n",
    "# Optimizer and LR scheduler\n",
    "def optim_optunet(model: nn.Module):\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer,\n",
    "        milestones=[15, 30],\n",
    "        gamma=0.5\n",
    "    )\n",
    "    return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "# Loss function\n",
    "def loss_optunet(model: nn.Module):\n",
    "    loss = ELBOLoss(\n",
    "        model=model,\n",
    "        inner_loss=nn.CrossEntropyLoss(),\n",
    "        kl_weight= 1/10000,\n",
    "        num_samples=3,\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions\n",
    "def load_optunet_model(version: int):\n",
    "    model = OptuNet()\n",
    "    path = Path(f\"models/mnist-optunet-0-8191/version_{version}.safetensors\")\n",
    "\n",
    "    if not path.exists():\n",
    "        raise ValueError(\"File does not exist\")\n",
    "\n",
    "    state_dict = load_file(path)\n",
    "    model.load_state_dict(state_dict=state_dict)\n",
    "    return model\n",
    "\n",
    "def load_trained_optunet(path):\n",
    "    checkpoint = torch.load(path)\n",
    "\n",
    "    # Filter out unwanted keys (e.g., those related to loss)\n",
    "    state_dict = {\n",
    "        k.replace(\"model.\", \"\"): v\n",
    "        for k, v in checkpoint[\"state_dict\"].items()\n",
    "        if not k.startswith(\"loss.\")\n",
    "    }\n",
    "    model = OptuNet()\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postreior models loaded: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_7052\\3861489287.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "# Compile posterior estimation models\n",
    "posterior_models = []\n",
    "\n",
    "for i in range(30):\n",
    "    path = Path(f\"models/trained_optunets/model_t{i}.pt\")\n",
    "    model = load_trained_optunet(path)\n",
    "    model = model.to(DEVICE) # Needed?\n",
    "    posterior_models.append(model)\n",
    "\n",
    "print(\"Postreior models loaded:\", len(posterior_models))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TUTrainer(\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    max_epochs=EPOCHS)\n",
    "\n",
    "# model\n",
    "model = OptuNet()#load_optunet_model(version=1000)\n",
    "\n",
    "# loss\n",
    "loss = ELBOLoss(\n",
    "    model=model,\n",
    "    inner_loss=nn.CrossEntropyLoss(),\n",
    "    kl_weight=1 / 10000,\n",
    "    num_samples=3,\n",
    ")\n",
    "\n",
    "routine = ClassificationRoutine(\n",
    "    model=model,\n",
    "    num_classes=datamodule.num_classes,\n",
    "    loss=loss,\n",
    "    optim_recipe=optim_optunet(model),\n",
    "    is_ensemble=True\n",
    ")\n",
    "\n",
    "trainer.fit(model=routine, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 100 # models to train\n",
    "\n",
    "for i in range(n_models):\n",
    "    model = OptuNet()\n",
    "\n",
    "    trainer = TUTrainer(\n",
    "        accelerator=\"gpu\",\n",
    "        enable_progress_bar=False,\n",
    "        max_epochs=EPOCHS)\n",
    "\n",
    "    routine = ClassificationRoutine(\n",
    "        model=model,\n",
    "        num_classes=datamodule.num_classes,\n",
    "        loss=loss,\n",
    "        optim_recipe=optim_optunet(model),\n",
    "        is_ensemble=True\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=routine, datamodule=datamodule)\n",
    "\n",
    "    # Save the trained model\n",
    "    save_path = Path(MODEL_PATH, f\"model_t{i+1}.pt\")\n",
    "    trainer.save_checkpoint(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:02<00:00, 77.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:02<00:00, 77.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">      Classification       </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     Acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          83.82%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    Brier     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.24618          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   Entropy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.64251          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     NLL      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.54042          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Calibration        </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     ECE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.05092          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     aECE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.05092          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\"> Selective Classification  </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    AUGRC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           3.48%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     AURC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           4.58%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Cov@5Risk   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          63.85%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Risk@80Cov  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           8.71%           </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Classification      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    Acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         83.82%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   Brier    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.24618         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  Entropy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.64251         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    NLL     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.54042         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Calibration       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    ECE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.05092         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    aECE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.05092         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSelective Classification \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   AUGRC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          3.48%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    AURC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          4.58%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Cov@5Risk  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         63.85%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Risk@80Cov \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          8.71%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = trainer.test(model=routine, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptuDrop(OptuNet):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.pool1(self.conv1(x)))  # First conv, max pooling, ReLU\n",
    "        x = self.relu(self.pool2(self.conv2(x)))  # Second conv, avg pooling, ReLU\n",
    "        x = torch.mean(x, dim=(2, 3)) # Global average pooling\n",
    "        x = self.dropout(self.fc1(x))  # Linear layer with dropout\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model            | OptuDrop         | 392    | train\n",
      "1 | loss             | CrossEntropyLoss | 0      | train\n",
      "2 | format_batch_fn  | Identity         | 0      | train\n",
      "3 | val_cls_metrics  | MetricCollection | 0      | train\n",
      "4 | test_cls_metrics | MetricCollection | 0      | train\n",
      "5 | test_id_entropy  | Entropy          | 0      | train\n",
      "6 | mixup            | Identity         | 0      | train\n",
      "--------------------------------------------------------------\n",
      "392       Trainable params\n",
      "0         Non-trainable params\n",
      "392       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "32        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:10<00:00, 91.55it/s, v_num=99, train_loss=1.690]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 938/938 [00:05<00:00, 161.97it/s, v_num=99, train_loss=1.460, Acc%=44.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 938/938 [00:06<00:00, 147.66it/s, v_num=99, train_loss=1.370, Acc%=72.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 938/938 [00:06<00:00, 136.12it/s, v_num=99, train_loss=0.996, Acc%=74.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 938/938 [00:06<00:00, 144.97it/s, v_num=99, train_loss=1.680, Acc%=74.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 938/938 [00:06<00:00, 139.50it/s, v_num=99, train_loss=1.060, Acc%=67.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 938/938 [00:06<00:00, 148.99it/s, v_num=99, train_loss=1.440, Acc%=75.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 938/938 [00:06<00:00, 151.31it/s, v_num=99, train_loss=1.280, Acc%=69.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 938/938 [00:05<00:00, 159.07it/s, v_num=99, train_loss=1.520, Acc%=74.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 938/938 [00:07<00:00, 132.41it/s, v_num=99, train_loss=1.370, Acc%=72.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 938/938 [00:07<00:00, 131.08it/s, v_num=99, train_loss=1.300, Acc%=75.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 938/938 [00:06<00:00, 138.74it/s, v_num=99, train_loss=1.310, Acc%=78.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 938/938 [00:06<00:00, 136.94it/s, v_num=99, train_loss=1.430, Acc%=77.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 938/938 [00:07<00:00, 132.03it/s, v_num=99, train_loss=1.300, Acc%=73.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 938/938 [00:07<00:00, 131.19it/s, v_num=99, train_loss=1.590, Acc%=77.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 938/938 [00:06<00:00, 143.57it/s, v_num=99, train_loss=1.360, Acc%=74.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 938/938 [00:07<00:00, 124.88it/s, v_num=99, train_loss=1.240, Acc%=77.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 938/938 [00:07<00:00, 129.06it/s, v_num=99, train_loss=1.180, Acc%=77.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 938/938 [00:06<00:00, 135.98it/s, v_num=99, train_loss=1.270, Acc%=76.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 938/938 [00:07<00:00, 124.06it/s, v_num=99, train_loss=1.150, Acc%=77.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 938/938 [00:06<00:00, 136.95it/s, v_num=99, train_loss=1.070, Acc%=78.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 938/938 [00:06<00:00, 137.06it/s, v_num=99, train_loss=1.330, Acc%=77.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 938/938 [00:08<00:00, 113.92it/s, v_num=99, train_loss=1.120, Acc%=78.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 938/938 [00:08<00:00, 113.71it/s, v_num=99, train_loss=1.120, Acc%=78.90]\n"
     ]
    }
   ],
   "source": [
    "model = OptuDrop()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "routine = ClassificationRoutine(\n",
    "    model=model,\n",
    "    num_classes=datamodule.num_classes,\n",
    "    loss=loss_fn,\n",
    "    optim_recipe=optim_optunet(model),\n",
    "    is_ensemble=False\n",
    ")\n",
    "\n",
    "trainer = TUTrainer(\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    max_epochs=EPOCHS\n",
    ")\n",
    "\n",
    "trainer.fit(model=routine, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:02<00:00, 77.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">      Classification       </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     Acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          78.95%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    Brier     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.32590          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   Entropy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          1.02363          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     NLL      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.69578          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Calibration        </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     ECE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.13991          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     aECE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.13981          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\"> Selective Classification  </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    AUGRC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           4.93%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     AURC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           6.46%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Cov@5Risk   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          53.47%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Risk@80Cov  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          12.79%           </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Classification      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    Acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         78.95%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   Brier    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.32590         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  Entropy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         1.02363         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    NLL     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.69578         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Calibration       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    ECE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.13991         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    aECE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.13981         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSelective Classification \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   AUGRC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          4.93%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    AURC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          6.46%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Cov@5Risk  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         53.47%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Risk@80Cov \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         12.79%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing\n",
    "results = trainer.test(model=routine, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viBNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train OptuNet for SWAG\n",
    "model = OptuNet()\n",
    "\n",
    "routine = ClassificationRoutine(\n",
    "    model=model,\n",
    "    num_classes=datamodule.num_classes,\n",
    "    loss=nn.CrossEntropyLoss(),\n",
    "    optim_recipe=optim_optunet(model),\n",
    "    is_ensemble=False # Single model (not ensemble here)\n",
    ")\n",
    "\n",
    "trainer = TUTrainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=2*EPOCHS, # Train twice as long for SWAG\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(model=routine, datamodule=datamodule)\n",
    "\n",
    "# Save checkpoints every 10 epochs from epoch 80 onward\n",
    "checkpoint_dir = Path(\"models/swag_checkpoints/\")\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "checkpoints = []\n",
    "for epoch in range(80, 121, 10):\n",
    "    checkpoint_path = checkpoint_dir / f\"model_epoch_{epoch}.pt\"\n",
    "    trainer.save_checkpoint(checkpoint_path)\n",
    "    checkpoints.append(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_7052\\236377326.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_7052\\1724321880.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "# Create the SWAG object\n",
    "swag_model = SWAG(\n",
    "    base=OptuNet,\n",
    "    max_num_models=20,\n",
    "    var_clamp=1e-30\n",
    ")\n",
    "\n",
    "# Add the collected checkpoints to the SWAG posterior\n",
    "for checkpoint_path in checkpoints:\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    swag_model.collect_model(load_trained_optunet(checkpoint_path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For consistent results\n",
    "class SWAGLightningWrapper(LightningModule):\n",
    "    def __init__(self, swag_model, num_samples=10, scale=0.1):\n",
    "        super().__init__()\n",
    "        self.swag_model = swag_model\n",
    "        self.num_samples = num_samples\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        preds = []\n",
    "        for _ in range(self.num_samples):\n",
    "            sampled_model = self.swag_model.sample(scale=self.scale)  # Sample from SWAG posterior\n",
    "            sampled_model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds.append(sampled_model(x))\n",
    "        return torch.stack(preds).mean(dim=0)  # Aggregate predictions\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        return self.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "No `test_step()` method defined to run `Trainer.test`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m swag_wrapper \u001b[38;5;241m=\u001b[39m SWAGLightningWrapper(\n\u001b[0;32m      3\u001b[0m     swag_model\u001b[38;5;241m=\u001b[39mswag_model,\n\u001b[0;32m      4\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m# Number of posterior samples as per the paper\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m        \u001b[38;5;66;03m# Scale parameter for SWAG sampling\u001b[39;00m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Test the SWAG model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswag_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSWAG Test Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:748\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:788\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    785\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    787\u001b[0m )\n\u001b[1;32m--> 788\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[0;32m    790\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:931\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector\u001b[38;5;241m.\u001b[39m_attach_model_callbacks()\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector\u001b[38;5;241m.\u001b[39m_attach_model_logging_functions()\n\u001b[1;32m--> 931\u001b[0m \u001b[43m_verify_loop_configurations\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;66;03m# SET UP THE TRAINER\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    936\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: setting up strategy environment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:41\u001b[0m, in \u001b[0;36m_verify_loop_configurations\u001b[1;34m(trainer)\u001b[0m\n\u001b[0;32m     39\u001b[0m     __verify_eval_loop_configuration(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mTESTING:\n\u001b[1;32m---> 41\u001b[0m     \u001b[43m__verify_eval_loop_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mPREDICTING:\n\u001b[0;32m     43\u001b[0m     __verify_eval_loop_configuration(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mh:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:106\u001b[0m, in \u001b[0;36m__verify_eval_loop_configuration\u001b[1;34m(model, stage)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_step:\n\u001b[0;32m    105\u001b[0m     trainer_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m stage\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()` method defined to run `Trainer.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# check legacy hooks are not present\u001b[39;00m\n\u001b[0;32m    109\u001b[0m epoch_end_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: No `test_step()` method defined to run `Trainer.test`."
     ]
    }
   ],
   "source": [
    "# Wrap SWAG model for testing\n",
    "swag_wrapper = SWAGLightningWrapper(\n",
    "    swag_model=swag_model,\n",
    "    num_samples=10,  # Number of posterior samples as per the paper\n",
    "    scale=0.1        # Scale parameter for SWAG sampling\n",
    ")\n",
    "\n",
    "# Test the SWAG model\n",
    "results = trainer.test(model=swag_wrapper, datamodule=datamodule)\n",
    "print(\"SWAG Test Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:01<00:00, 97.30it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">      Classification       </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     Acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          82.68%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    Brier     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.25886          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   Entropy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.67752          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     NLL      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.57040          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Calibration        </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     ECE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.05344          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     aECE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.05416          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\"> Selective Classification  </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    AUGRC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           3.76%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     AURC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           5.07%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Cov@5Risk   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          63.02%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Risk@80Cov  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           9.30%           </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Classification      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    Acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         82.68%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   Brier    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.25886         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  Entropy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.67752         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    NLL     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.57040         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Calibration       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    ECE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.05344         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    aECE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.05416         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSelective Classification \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   AUGRC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          3.76%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    AURC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          5.07%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Cov@5Risk  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         63.02%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Risk@80Cov \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          9.30%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/cal/ECE': 0.05344296991825104,\n",
       "  'test/cal/aECE': 0.05416256561875343,\n",
       "  'test/cls/Acc': 0.8267999887466431,\n",
       "  'test/cls/Brier': 0.25886476039886475,\n",
       "  'test/cls/NLL': 0.570395290851593,\n",
       "  'test/sc/AUGRC': 0.037595879286527634,\n",
       "  'test/sc/AURC': 0.050700489431619644,\n",
       "  'test/sc/Cov@5Risk': 0.6302000284194946,\n",
       "  'test/sc/Risk@80Cov': 0.09300000220537186,\n",
       "  'test/cls/Entropy': 0.6775237917900085}]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=routine, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model            | OptuNet          | 392    | train\n",
      "1 | loss             | CrossEntropyLoss | 0      | train\n",
      "2 | format_batch_fn  | Identity         | 0      | train\n",
      "3 | val_cls_metrics  | MetricCollection | 0      | train\n",
      "4 | test_cls_metrics | MetricCollection | 0      | train\n",
      "5 | test_id_entropy  | Entropy          | 0      | train\n",
      "6 | mixup            | Identity         | 0      | train\n",
      "--------------------------------------------------------------\n",
      "392       Trainable params\n",
      "0         Non-trainable params\n",
      "392       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:09<00:00, 96.90it/s, v_num=57, train_loss=1.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 938/938 [00:05<00:00, 160.81it/s, v_num=57, train_loss=1.070, Acc%=52.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 938/938 [00:06<00:00, 143.72it/s, v_num=57, train_loss=1.740, Acc%=59.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 938/938 [00:06<00:00, 139.31it/s, v_num=57, train_loss=0.841, Acc%=53.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 938/938 [00:06<00:00, 144.08it/s, v_num=57, train_loss=1.010, Acc%=73.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 938/938 [00:06<00:00, 148.42it/s, v_num=57, train_loss=1.680, Acc%=65.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 938/938 [00:06<00:00, 148.22it/s, v_num=57, train_loss=0.599, Acc%=76.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 938/938 [00:06<00:00, 143.06it/s, v_num=57, train_loss=0.713, Acc%=79.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 938/938 [00:06<00:00, 138.37it/s, v_num=57, train_loss=1.010, Acc%=79.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 938/938 [00:06<00:00, 144.06it/s, v_num=57, train_loss=0.803, Acc%=80.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 938/938 [00:06<00:00, 144.23it/s, v_num=57, train_loss=0.867, Acc%=74.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 938/938 [00:06<00:00, 151.63it/s, v_num=57, train_loss=1.390, Acc%=80.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:06<00:00, 152.22it/s, v_num=57, train_loss=0.696, Acc%=82.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 938/938 [00:07<00:00, 123.66it/s, v_num=57, train_loss=0.513, Acc%=83.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 938/938 [00:06<00:00, 134.27it/s, v_num=57, train_loss=1.040, Acc%=75.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 938/938 [00:07<00:00, 120.40it/s, v_num=57, train_loss=0.573, Acc%=83.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 938/938 [00:06<00:00, 151.51it/s, v_num=57, train_loss=0.517, Acc%=81.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 938/938 [00:06<00:00, 143.95it/s, v_num=57, train_loss=0.956, Acc%=83.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 938/938 [00:06<00:00, 155.05it/s, v_num=57, train_loss=0.769, Acc%=79.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 938/938 [00:06<00:00, 146.01it/s, v_num=57, train_loss=0.626, Acc%=81.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 938/938 [00:06<00:00, 145.88it/s, v_num=57, train_loss=0.469, Acc%=82.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 938/938 [00:06<00:00, 147.30it/s, v_num=57, train_loss=1.710, Acc%=83.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 938/938 [00:05<00:00, 158.17it/s, v_num=57, train_loss=0.833, Acc%=76.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 938/938 [00:06<00:00, 154.98it/s, v_num=57, train_loss=0.886, Acc%=75.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 938/938 [00:06<00:00, 140.42it/s, v_num=57, train_loss=0.419, Acc%=81.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 938/938 [00:06<00:00, 140.28it/s, v_num=57, train_loss=0.419, Acc%=81.80]\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = OptuNet()\n",
    "\n",
    "def scheduler_laplace(optimizer):\n",
    "    return MultiStepLR(\n",
    "        optimizer,\n",
    "        milestones=[15, 30],\n",
    "        gamma=0.5\n",
    "    )\n",
    "\n",
    "# Routine\n",
    "loss_fn = nn.CrossEntropyLoss()  # Standard cross-entropy loss\n",
    "routine = ClassificationRoutine(\n",
    "    model=model,\n",
    "    num_classes=datamodule.num_classes,\n",
    "    loss=loss_fn,\n",
    "    optim_recipe=optim_optunet(model),\n",
    "    # scheduler_recipe=scheduler_laplace,\n",
    "    is_ensemble=False\n",
    ")\n",
    "\n",
    "# Train the model to MAP estimate\n",
    "trainer = TUTrainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=EPOCHS,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "trainer.fit(model=routine, datamodule=datamodule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\laplace\\baselaplace.py:435: UserWarning: By default `link_approx` is `probit`. Make sure to set it equals to the way you want to call `la(test_data, pred_type=..., link_approx=...)`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Apply Laplace approximation\n",
    "laplace_model = Laplace(model, likelihood='classification', subset_of_weights='last_layer', hessian_structure='full')\n",
    "laplace_model.fit(datamodule.train_dataloader())  # Fit the Laplace model on training data\n",
    "laplace_model.optimize_prior_precision()  # Optimize prior precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:01<00:00, 88.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">      Classification       </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     Acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          81.76%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    Brier     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.27002          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   Entropy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.58099          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     NLL      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.57028          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Calibration        </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     ECE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.02009          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     aECE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.02005          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\"> Selective Classification  </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    AUGRC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           4.41%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     AURC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           5.96%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Cov@5Risk   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          52.26%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Risk@80Cov  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          10.91%           </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Classification      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    Acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         81.76%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   Brier    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.27002         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  Entropy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.58099         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    NLL     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.57028         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Calibration       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    ECE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.02009         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    aECE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.02005         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSelective Classification \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   AUGRC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          4.41%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    AURC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          5.96%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Cov@5Risk  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         52.26%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Risk@80Cov \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         10.91%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace Test Results: [{'test/cal/ECE': 0.020094318315386772, 'test/cal/aECE': 0.02005278505384922, 'test/cls/Acc': 0.8176000118255615, 'test/cls/Brier': 0.2700233459472656, 'test/cls/NLL': 0.5702756643295288, 'test/sc/AUGRC': 0.04408245161175728, 'test/sc/AURC': 0.05958322063088417, 'test/sc/Cov@5Risk': 0.5226000547409058, 'test/sc/Risk@80Cov': 0.10912500321865082, 'test/cls/Entropy': 0.5809915661811829}]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "results = trainer.test(model=routine, datamodule=datamodule)\n",
    "print(\"Laplace Test Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGHMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pSGLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_sampling(model, data_loader, num_samples=50):\n",
    "    \"\"\"\n",
    "    Perform Monte Carlo Dropout sampling on the model.\n",
    "    Args:\n",
    "        model (nn.Module): Trained OptuDrop model with dropout.\n",
    "        data_loader (DataLoader): DataLoader for test data.\n",
    "        num_samples (int): Number of MC samples.\n",
    "    Returns:\n",
    "        np.ndarray: Array of predictions from all samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        sampled_preds = []\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                sampled_preds.append(outputs.cpu().numpy())\n",
    "        predictions.append(np.concatenate(sampled_preds, axis=0))\n",
    "\n",
    "    return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_weights(models):\n",
    "    \"\"\"\n",
    "    Extract model weights to use as samples for posterior comparison.\n",
    "    Args:\n",
    "        models (list): List of trained models.\n",
    "    Returns:\n",
    "        np.ndarray: Flattened weight arrays for each model.\n",
    "    \"\"\"\n",
    "    weight_samples = []\n",
    "    for model in models:\n",
    "        weights = []\n",
    "        for param in model.parameters():\n",
    "            weights.append(param.detach().cpu().numpy().flatten())\n",
    "        weight_samples.append(np.concatenate(weights))\n",
    "    return np.array(weight_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_model_predictions(models, data_loader):\n",
    "    \"\"\"\n",
    "    Generate predictions from trained models to form the target posterior in prediction space.\n",
    "    Args:\n",
    "        models (list): List of trained models.\n",
    "        data_loader (DataLoader): DataLoader for test data.\n",
    "    Returns:\n",
    "        np.ndarray: Array of predictions for all models.\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)  # Logits\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "        all_predictions.append(np.concatenate(preds, axis=0))  # Combine batches\n",
    "\n",
    "    return np.array(all_predictions)  # Shape: (num_models, num_datapoints, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample model posterior\n",
    "dropout_samples = monte_carlo_sampling(model, datamodule.test_dataloader()[0], num_samples=50)\n",
    "dropout_samples_flat = dropout_samples.reshape(dropout_samples.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target posterior\n",
    "target_predictions = target_model_predictions(posterior_models, datamodule.test_dataloader()[0])\n",
    "# target_posterior_samples = extract_model_weights(posterior_models)\n",
    "target_posterior_samples = target_predictions.reshape(30, -1)  # Shape: (30, 10000 * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 10000, 10)\n",
      "(50, 100000)\n",
      "(30, 100000)\n"
     ]
    }
   ],
   "source": [
    "print(dropout_samples.shape)\n",
    "print(dropout_samples_flat.shape)\n",
    "print(target_posterior_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -5.2107353   -0.95131683   3.909545     3.1656933   -1.405637\n",
      "  -5.1011004  -10.773238    10.522338     0.7293606    4.6583385 ]\n",
      "[-1.2191579  -0.7090111   4.4251957   1.871046   -0.4916743  -5.1103597\n",
      " -7.6788454   7.466436   -1.2615769  -0.34655005]\n"
     ]
    }
   ],
   "source": [
    "print(dropout_samples_flat[0][:10])\n",
    "print(target_posterior_samples[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMD Score: [0.84391623 0.79542857 0.73696664 0.67138301 0.60202212 0.53211281\n",
      " 0.4643618  0.40076836 0.34261158 0.29054258 0.0176627  0.58616206\n",
      " 0.88334639 0.92725724 0.93261007 0.93324742 0.93332312 0.93326705\n",
      " 0.64932177 0.11887361]\n"
     ]
    }
   ],
   "source": [
    "# Use mmdagg function\n",
    "mmd_score = mmdagg(\n",
    "    X=dropout_samples_flat,\n",
    "    Y=target_posterior_samples,\n",
    "    alpha=0.05,\n",
    "    kernel=\"laplace_gaussian\",\n",
    "    number_bandwidths=10,\n",
    "    weights_type=\"uniform\",\n",
    "    B1=2000,\n",
    "    B2=2000,\n",
    "    B3=50,\n",
    "    seed=42424242,\n",
    ")\n",
    "print(\"MMD Score:\", mmd_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMD avg: 0.6297592563799301\n",
      "MMD Shape: (20,)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"MMD avg:\", np.mean(mmd_score))\n",
    "print(\"MMD Shape:\", mmd_score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = trainer.test(model=routine, datamodule=datamodule)[0]\n",
    "\n",
    "# Example inputs\n",
    "probs = torch.softmax(model_outputs, dim=1)  # Mean probabilities from SWAG samples\n",
    "individual_entropies = compute_entropies(model_samples)  # Implement entropy computation for sampled models\n",
    "in_confidences = torch.max(probs, dim=1)[0]  # Confidence scores for in-distribution data\n",
    "ood_confidences = torch.max(ood_probs, dim=1)[0]  # Replace with OOD predictions\n",
    "labels = torch.cat([torch.ones(len(in_confidences)), torch.zeros(len(ood_confidences))])\n",
    "\n",
    "# Compute scores\n",
    "scores = score_methods(test_outputs, probs, individual_entropies, in_confidences, ood_confidences, labels)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels = [], []\n",
    "\n",
    "for batch in datamodule.test_dataloader()[0]:\n",
    "    images, true_labels = batch\n",
    "    with torch.no_grad():\n",
    "        probs = model(images)  # Assuming the model outputs probabilities\n",
    "        predictions.append(probs)\n",
    "        labels.append(true_labels)\n",
    "\n",
    "predictions = torch.cat(predictions).numpy()\n",
    "labels = torch.cat(labels).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPR for each class: {0: 0.5868727984354928, 1: 0.9735599255356758, 2: 0.7819963789640088, 3: 0.6210323998041789, 4: 0.9572730746676985, 5: 0.8067717806007965, 6: 0.921096030318654, 7: 0.8066637236494992, 8: 0.5346925620695928, 9: 0.686130600547784}\n",
      "Mean AUPR: 0.7676089274593382\n"
     ]
    }
   ],
   "source": [
    "n_classes = predictions.shape[1]  # Number of classes\n",
    "precision = {}\n",
    "recall = {}\n",
    "aupr = {}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    # Binarize the labels for class i\n",
    "    binary_labels = (labels == i).astype(int)\n",
    "    precision[i], recall[i], _ = precision_recall_curve(binary_labels, predictions[:, i])\n",
    "    aupr[i] = auc(recall[i], precision[i])\n",
    "\n",
    "# Optional: Aggregate AUPR\n",
    "mean_aupr = np.mean(list(aupr.values()))\n",
    "print(f\"AUPR for each class: {aupr}\")\n",
    "print(f\"Mean AUPR: {mean_aupr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aupr(labels, scores):\n",
    "    \"\"\"\n",
    "    Computes Area Under Precision-Recall Curve (AUPR).\n",
    "    Args:\n",
    "        labels (torch.Tensor): Ground truth labels (1 for in-distribution, 0 for OOD).\n",
    "        scores (torch.Tensor): Confidence scores.\n",
    "    Returns:\n",
    "        float: AUPR.\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(labels.cpu().numpy(), scores.cpu().numpy())\n",
    "    return auc(recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPR95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for each class: {0: 0.9461464319652472, 1: 0.9950144979389818, 2: 0.9628590068737078, 3: 0.944034350598575, 4: 0.99230256391494, 5: 0.9727843604695817, 6: 0.9862095652900706, 7: 0.9559805386754469, 8: 0.9133061186233155, 9: 0.9450068943516803, 'micro': 0.959201013888889}\n",
      "Micro-averaged ROC AUC: 0.959201013888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `labels` are integers representing classes and `predictions` are probabilities\n",
    "n_classes = predictions.shape[1]  # Number of classes\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    # Binarize the labels for class `i`\n",
    "    binary_labels = (labels == i).astype(int)\n",
    "    fpr[i], tpr[i], _ = roc_curve(binary_labels, predictions[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Optional: Compute micro-average ROC curve and AUC\n",
    "labels_one_hot = np.eye(n_classes)[labels]  # Convert labels to one-hot encoding\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels_one_hot.ravel(), predictions.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Print the results\n",
    "print(f\"ROC AUC for each class: {roc_auc}\")\n",
    "print(f\"Micro-averaged ROC AUC: {roc_auc['micro']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fpr95(in_confidences, ood_confidences):\n",
    "    \"\"\"\n",
    "    Computes False Positive Rate at 95% Recall.\n",
    "    Args:\n",
    "        in_confidences (torch.Tensor): Confidence scores for in-distribution data.\n",
    "        ood_confidences (torch.Tensor): Confidence scores for out-of-distribution data.\n",
    "    Returns:\n",
    "        float: FPR at 95% recall.\n",
    "    \"\"\"\n",
    "    labels = torch.cat([torch.ones_like(in_confidences), torch.zeros_like(ood_confidences)])\n",
    "    scores = torch.cat([in_confidences, ood_confidences])\n",
    "    fpr, tpr, _ = roc_curve(labels.cpu().numpy(), scores.cpu().numpy())\n",
    "    idx_95 = (tpr >= 0.95).nonzero()[0][0]\n",
    "    return fpr[idx_95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8439\n"
     ]
    }
   ],
   "source": [
    "# For multi-class classification\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
