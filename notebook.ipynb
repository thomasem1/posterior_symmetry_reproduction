{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior Symmetry Reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR, LambdaLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_uncertainty import TUTrainer\n",
    "from torch_uncertainty.datamodules import MNISTDataModule\n",
    "from torch_uncertainty.losses import ELBOLoss\n",
    "from torch_uncertainty.models.lenet import bayesian_lenet, lenet\n",
    "from torch_uncertainty.models import mc_dropout\n",
    "from torch_uncertainty.routines import ClassificationRoutine\n",
    "from lightning.pytorch import LightningModule\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "from laplace import Laplace\n",
    "from utils.swa_gaussian.swag.posteriors import SWAG\n",
    "import utils.swa_gaussian.swag.posteriors as swag_posteriors\n",
    "from utils.posterior_symmetry.mmd.mmd_torch import mmdagg\n",
    "from utils.posterior_symmetry.symmetries.permutation import Permuter\n",
    "# from utils.posterior_symmetry.symmetries.scale import Scaler # ANVÄNDER torch_symmetry SOM INTE FINNS ELLER ??\n",
    "from utils.bayes_neural_networks.src.Stochastic_Gradient_HMC_SA.optimizers import H_SA_SGHMC\n",
    "\n",
    "from pathlib import Path\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_PATH = \"data\"\n",
    "MODEL_PATH = Path(\"models\", \"trained_optunets\")\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Parameters from paper\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.04\n",
    "WEIGHT_DECAY = 2e-4\n",
    "\n",
    "## Method params\n",
    "DROPOUT_RATE = 0.2 # last layer dropout rate\n",
    "\n",
    "# Models\n",
    "NMODELS = 100\n",
    "ENSEMBLE_MODELS = 10 # number of models to use in ensemble\n",
    "N_SAMPLES = 100 # number of samples to draw from model\n",
    "\n",
    "MC_SAMPLES = 3 # Posterior samples for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "root = Path(DATA_PATH)\n",
    "datamodule = MNISTDataModule(root=root, batch_size=BATCH_SIZE, eval_ood=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OptuNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptuNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Add layers for OptuNet (use Section C.2.1 from the paper for details)\n",
    "        # Layers: Conv2D (out_ch=2, ks=4, groups=1) -> Max Pooling (ks=3, stride=3) -> ReLU -> Conv2D (out_ch=10, ks=5, groups=2) -> Average Pooling -> ReLU -> Linear 10x10\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=4, groups=1, bias=False)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=10, kernel_size=5, groups=2, bias=False)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.pool1(self.conv1(x)))  # First conv, max pooling, ReLU\n",
    "        x = self.relu(self.pool2(self.conv2(x)))  # Second conv, avg pooling, ReLU\n",
    "        x = torch.mean(x, dim=(2, 3))\n",
    "        x = self.fc1(x)  # Linear layer\n",
    "        return x\n",
    "\n",
    "# Optimizer and LR scheduler\n",
    "def optim_optunet(model: nn.Module):\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer,\n",
    "        milestones=[15, 30],\n",
    "        gamma=0.5\n",
    "    )\n",
    "    return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "# Loss function\n",
    "def loss_optunet(model: nn.Module):\n",
    "    loss = ELBOLoss(\n",
    "        model=model,\n",
    "        inner_loss=nn.CrossEntropyLoss(),\n",
    "        kl_weight= 1/10000,\n",
    "        num_samples=3,\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions\n",
    "def load_optunet_model(version: int):\n",
    "    model = OptuNet()\n",
    "    path = Path(f\"models/mnist-optunet-0-8191/version_{version}.safetensors\")\n",
    "\n",
    "    if not path.exists():\n",
    "        raise ValueError(\"File does not exist\")\n",
    "\n",
    "    state_dict = load_file(path)\n",
    "    model.load_state_dict(state_dict=state_dict)\n",
    "    return model\n",
    "\n",
    "def load_trained_optunet(path):\n",
    "    checkpoint = torch.load(path)\n",
    "\n",
    "    # Filter out unwanted keys (e.g., those related to loss)\n",
    "    state_dict = {\n",
    "        k.replace(\"model.\", \"\"): v\n",
    "        for k, v in checkpoint[\"state_dict\"].items()\n",
    "        if not k.startswith(\"loss.\")\n",
    "    }\n",
    "    model = OptuNet()\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_optunets(n_models = 100, start_idx = 0, tag=\"\"):\n",
    "    # tag (str) is used to tag save name with specific tag\n",
    "\n",
    "    # Train n_models OptuNets\n",
    "    for i in range(start_idx, n_models):\n",
    "        model = OptuNet()\n",
    "\n",
    "        trainer = TUTrainer(\n",
    "            accelerator=\"gpu\",\n",
    "            enable_progress_bar=False,\n",
    "            max_epochs=EPOCHS)\n",
    "        \n",
    "        # loss\n",
    "        loss = ELBOLoss(\n",
    "            model=model,\n",
    "            inner_loss=nn.CrossEntropyLoss(),\n",
    "            kl_weight=1/10000,\n",
    "            num_samples=3,\n",
    "        )\n",
    "\n",
    "        routine = ClassificationRoutine(\n",
    "            model=model,\n",
    "            num_classes=datamodule.num_classes,\n",
    "            loss=loss,\n",
    "            optim_recipe=optim_optunet(model),\n",
    "            is_ensemble=True\n",
    "        )\n",
    "\n",
    "        trainer.fit(model=routine, datamodule=datamodule)\n",
    "\n",
    "        # Save the trained model\n",
    "        save_path = Path(MODEL_PATH, f\"model_{tag}{i+1}.pt\")\n",
    "        trainer.save_checkpoint(save_path)\n",
    "    \n",
    "    print(f\"Trained {n_models} models. Saved to {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_optunets(n_models=NMODELS, start_idx=30, tag=\"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_cosine_scheduler(model, warmup_steps, total_steps, min_lr=0, max_lr=0.04):\n",
    "    # Define the learning rate scheduler as a Lambda function\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_steps:\n",
    "            # Linear warmup: Increase from 0 to max_lr\n",
    "            return float(epoch) / float(max(1, warmup_steps))\n",
    "        else:\n",
    "            # Cosine decay after warmup\n",
    "            progress = (epoch - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "            return min_lr + 0.5 * (max_lr - min_lr) * (1 + torch.cos(torch.pi * progress))\n",
    "\n",
    "    optimizer = optim_optunet(model)[\"optimizer\"]\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(model, path):\n",
    "    # Ensure path exists\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_models(n_models = 100, start_idx = 0, tag=\"\"):\n",
    "    posterior_models = []\n",
    "\n",
    "    for i in range(start_idx, n_models):\n",
    "        path = Path(MODEL_PATH, f\"model_{tag}{i+1}.pt\")\n",
    "        model = load_trained_optunet(path)\n",
    "        model = model.to(DEVICE) # Needed?\n",
    "        posterior_models.append(model)\n",
    "    \n",
    "    print(f\"Loaded {len(posterior_models)} models\")\n",
    "    return posterior_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_2164\\3394468100.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 models\n"
     ]
    }
   ],
   "source": [
    "posterior_models = load_trained_models(n_models=30, tag=\"t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptuDrop(OptuNet):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(super().forward(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model            | OptuDrop         | 392    | train\n",
      "1 | loss             | CrossEntropyLoss | 0      | train\n",
      "2 | format_batch_fn  | Identity         | 0      | train\n",
      "3 | val_cls_metrics  | MetricCollection | 0      | train\n",
      "4 | test_cls_metrics | MetricCollection | 0      | train\n",
      "5 | test_id_entropy  | Entropy          | 0      | train\n",
      "6 | mixup            | Identity         | 0      | train\n",
      "--------------------------------------------------------------\n",
      "392       Trainable params\n",
      "0         Non-trainable params\n",
      "392       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "32        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:09<00:00, 94.13it/s, v_num=106, train_loss=1.910]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 938/938 [00:05<00:00, 163.68it/s, v_num=106, train_loss=1.650, Acc%=41.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 938/938 [00:06<00:00, 154.54it/s, v_num=106, train_loss=1.680, Acc%=48.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 938/938 [00:06<00:00, 147.46it/s, v_num=106, train_loss=1.620, Acc%=65.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 938/938 [00:05<00:00, 157.83it/s, v_num=106, train_loss=1.710, Acc%=67.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 938/938 [00:05<00:00, 164.73it/s, v_num=106, train_loss=1.250, Acc%=65.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 938/938 [00:05<00:00, 158.81it/s, v_num=106, train_loss=1.640, Acc%=71.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 938/938 [00:05<00:00, 160.11it/s, v_num=106, train_loss=1.310, Acc%=68.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 938/938 [00:06<00:00, 151.05it/s, v_num=106, train_loss=1.070, Acc%=71.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 938/938 [00:06<00:00, 143.31it/s, v_num=106, train_loss=1.230, Acc%=74.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 938/938 [00:05<00:00, 159.40it/s, v_num=106, train_loss=1.050, Acc%=72.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 938/938 [00:05<00:00, 162.21it/s, v_num=106, train_loss=1.260, Acc%=76.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 938/938 [00:06<00:00, 144.78it/s, v_num=106, train_loss=1.490, Acc%=68.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 938/938 [00:06<00:00, 142.87it/s, v_num=106, train_loss=0.844, Acc%=75.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 938/938 [00:06<00:00, 151.33it/s, v_num=106, train_loss=0.933, Acc%=74.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 938/938 [00:06<00:00, 153.40it/s, v_num=106, train_loss=1.250, Acc%=73.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 938/938 [00:06<00:00, 148.41it/s, v_num=106, train_loss=1.200, Acc%=77.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 938/938 [00:06<00:00, 144.26it/s, v_num=106, train_loss=1.040, Acc%=76.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 938/938 [00:06<00:00, 150.53it/s, v_num=106, train_loss=1.030, Acc%=77.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 938/938 [00:06<00:00, 145.90it/s, v_num=106, train_loss=0.948, Acc%=76.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 938/938 [00:06<00:00, 154.13it/s, v_num=106, train_loss=1.170, Acc%=79.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 938/938 [00:06<00:00, 142.56it/s, v_num=106, train_loss=1.210, Acc%=78.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 938/938 [00:07<00:00, 128.11it/s, v_num=106, train_loss=1.190, Acc%=83.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 938/938 [00:07<00:00, 127.97it/s, v_num=106, train_loss=1.190, Acc%=83.20]\n"
     ]
    }
   ],
   "source": [
    "model = OptuDrop()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "routine = ClassificationRoutine(\n",
    "    model=model,\n",
    "    num_classes=datamodule.num_classes,\n",
    "    loss=loss_fn,\n",
    "    optim_recipe=optim_optunet(model),\n",
    "    is_ensemble=False\n",
    ")\n",
    "\n",
    "trainer = TUTrainer(\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    max_epochs=EPOCHS\n",
    ")\n",
    "\n",
    "trainer.fit(model=routine, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:01<00:00, 88.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">      Classification       </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     Acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          83.21%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    Brier     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.26384          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   Entropy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.81296          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     NLL      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.56206          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Calibration        </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     ECE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.10622          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     aECE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.10622          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\"> Selective Classification  </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    AUGRC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           3.50%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     AURC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           4.51%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Cov@5Risk   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          65.15%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Risk@80Cov  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           8.86%           </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Classification      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    Acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         83.21%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   Brier    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.26384         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  Entropy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.81296         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    NLL     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.56206         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Calibration       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    ECE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.10622         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    aECE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.10622         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSelective Classification \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   AUGRC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          3.50%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    AURC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          4.51%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Cov@5Risk  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         65.15%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Risk@80Cov \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          8.86%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing\n",
    "results = trainer.test(model=routine, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viBNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train OptuNet for SWAG\n",
    "model = OptuNet()\n",
    "\n",
    "routine = ClassificationRoutine(\n",
    "    model=model,\n",
    "    num_classes=datamodule.num_classes,\n",
    "    loss=nn.CrossEntropyLoss(),\n",
    "    optim_recipe=optim_optunet(model),\n",
    "    is_ensemble=False # Single model (not ensemble here)\n",
    ")\n",
    "\n",
    "trainer = TUTrainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=2*EPOCHS, # Train twice as long for SWAG\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(model=routine, datamodule=datamodule)\n",
    "\n",
    "# Save checkpoints every 10 epochs from epoch 80 onward\n",
    "checkpoint_dir = Path(\"models/swag_checkpoints/\")\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "checkpoints = []\n",
    "for epoch in range(80, 121, 10):\n",
    "    checkpoint_path = checkpoint_dir / f\"model_epoch_{epoch}.pt\"\n",
    "    trainer.save_checkpoint(checkpoint_path)\n",
    "    checkpoints.append(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_7052\\236377326.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n",
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_7052\\1724321880.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "# Create the SWAG object\n",
    "swag_model = SWAG(\n",
    "    base=OptuNet,\n",
    "    max_num_models=20,\n",
    "    var_clamp=1e-30\n",
    ")\n",
    "\n",
    "# Add the collected checkpoints to the SWAG posterior\n",
    "for checkpoint_path in checkpoints:\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    swag_model.collect_model(load_trained_optunet(checkpoint_path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For consistent results\n",
    "class SWAGLightningWrapper(LightningModule):\n",
    "    def __init__(self, swag_model, num_samples=10, scale=0.1):\n",
    "        super().__init__()\n",
    "        self.swag_model = swag_model\n",
    "        self.num_samples = num_samples\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        preds = []\n",
    "        for _ in range(self.num_samples):\n",
    "            sampled_model = self.swag_model.sample(scale=self.scale)  # Sample from SWAG posterior\n",
    "            sampled_model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds.append(sampled_model(x))\n",
    "        return torch.stack(preds).mean(dim=0)  # Aggregate predictions\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        return self.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap SWAG model for testing\n",
    "swag_wrapper = SWAGLightningWrapper(\n",
    "    swag_model=swag_model,\n",
    "    num_samples=10,  # Number of posterior samples as per the paper\n",
    "    scale=0.1        # Scale parameter for SWAG sampling\n",
    ")\n",
    "\n",
    "# Test the SWAG model\n",
    "results = trainer.test(model=swag_wrapper, datamodule=datamodule)\n",
    "print(\"SWAG Test Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:01<00:00, 97.30it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">      Classification       </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     Acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          82.68%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    Brier     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.25886          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   Entropy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.67752          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     NLL      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.57040          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Calibration        </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     ECE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.05344          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     aECE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.05416          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\"> Selective Classification  </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    AUGRC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           3.76%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     AURC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           5.07%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Cov@5Risk   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          63.02%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Risk@80Cov  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           9.30%           </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Classification      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    Acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         82.68%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   Brier    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.25886         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  Entropy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.67752         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    NLL     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.57040         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Calibration       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    ECE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.05344         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    aECE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.05416         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSelective Classification \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   AUGRC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          3.76%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    AURC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          5.07%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Cov@5Risk  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         63.02%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Risk@80Cov \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          9.30%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/cal/ECE': 0.05344296991825104,\n",
       "  'test/cal/aECE': 0.05416256561875343,\n",
       "  'test/cls/Acc': 0.8267999887466431,\n",
       "  'test/cls/Brier': 0.25886476039886475,\n",
       "  'test/cls/NLL': 0.570395290851593,\n",
       "  'test/sc/AUGRC': 0.037595879286527634,\n",
       "  'test/sc/AURC': 0.050700489431619644,\n",
       "  'test/sc/Cov@5Risk': 0.6302000284194946,\n",
       "  'test/sc/Risk@80Cov': 0.09300000220537186,\n",
       "  'test/cls/Entropy': 0.6775237917900085}]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=routine, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model            | OptuNet          | 392    | train\n",
      "1 | loss             | CrossEntropyLoss | 0      | train\n",
      "2 | format_batch_fn  | Identity         | 0      | train\n",
      "3 | val_cls_metrics  | MetricCollection | 0      | train\n",
      "4 | test_cls_metrics | MetricCollection | 0      | train\n",
      "5 | test_id_entropy  | Entropy          | 0      | train\n",
      "6 | mixup            | Identity         | 0      | train\n",
      "--------------------------------------------------------------\n",
      "392       Trainable params\n",
      "0         Non-trainable params\n",
      "392       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:09<00:00, 96.90it/s, v_num=57, train_loss=1.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 938/938 [00:05<00:00, 160.81it/s, v_num=57, train_loss=1.070, Acc%=52.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 938/938 [00:06<00:00, 143.72it/s, v_num=57, train_loss=1.740, Acc%=59.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 938/938 [00:06<00:00, 139.31it/s, v_num=57, train_loss=0.841, Acc%=53.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 938/938 [00:06<00:00, 144.08it/s, v_num=57, train_loss=1.010, Acc%=73.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 938/938 [00:06<00:00, 148.42it/s, v_num=57, train_loss=1.680, Acc%=65.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 938/938 [00:06<00:00, 148.22it/s, v_num=57, train_loss=0.599, Acc%=76.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 938/938 [00:06<00:00, 143.06it/s, v_num=57, train_loss=0.713, Acc%=79.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 938/938 [00:06<00:00, 138.37it/s, v_num=57, train_loss=1.010, Acc%=79.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 938/938 [00:06<00:00, 144.06it/s, v_num=57, train_loss=0.803, Acc%=80.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 938/938 [00:06<00:00, 144.23it/s, v_num=57, train_loss=0.867, Acc%=74.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 938/938 [00:06<00:00, 151.63it/s, v_num=57, train_loss=1.390, Acc%=80.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:06<00:00, 152.22it/s, v_num=57, train_loss=0.696, Acc%=82.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 938/938 [00:07<00:00, 123.66it/s, v_num=57, train_loss=0.513, Acc%=83.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 938/938 [00:06<00:00, 134.27it/s, v_num=57, train_loss=1.040, Acc%=75.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 938/938 [00:07<00:00, 120.40it/s, v_num=57, train_loss=0.573, Acc%=83.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 938/938 [00:06<00:00, 151.51it/s, v_num=57, train_loss=0.517, Acc%=81.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 938/938 [00:06<00:00, 143.95it/s, v_num=57, train_loss=0.956, Acc%=83.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 938/938 [00:06<00:00, 155.05it/s, v_num=57, train_loss=0.769, Acc%=79.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 938/938 [00:06<00:00, 146.01it/s, v_num=57, train_loss=0.626, Acc%=81.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 938/938 [00:06<00:00, 145.88it/s, v_num=57, train_loss=0.469, Acc%=82.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 938/938 [00:06<00:00, 147.30it/s, v_num=57, train_loss=1.710, Acc%=83.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 938/938 [00:05<00:00, 158.17it/s, v_num=57, train_loss=0.833, Acc%=76.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 938/938 [00:06<00:00, 154.98it/s, v_num=57, train_loss=0.886, Acc%=75.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 938/938 [00:06<00:00, 140.42it/s, v_num=57, train_loss=0.419, Acc%=81.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 938/938 [00:06<00:00, 140.28it/s, v_num=57, train_loss=0.419, Acc%=81.80]\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = OptuNet()\n",
    "\n",
    "def scheduler_laplace(optimizer):\n",
    "    return MultiStepLR(\n",
    "        optimizer,\n",
    "        milestones=[15, 30],\n",
    "        gamma=0.5\n",
    "    )\n",
    "\n",
    "# Routine\n",
    "loss_fn = nn.CrossEntropyLoss()  # Standard cross-entropy loss\n",
    "routine = ClassificationRoutine(\n",
    "    model=model,\n",
    "    num_classes=datamodule.num_classes,\n",
    "    loss=loss_fn,\n",
    "    optim_recipe=optim_optunet(model),\n",
    "    # scheduler_recipe=scheduler_laplace,\n",
    "    is_ensemble=False\n",
    ")\n",
    "\n",
    "# Train the model to MAP estimate\n",
    "trainer = TUTrainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=EPOCHS,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "trainer.fit(model=routine, datamodule=datamodule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\dev\\school\\DD2412 DLA\\posterior_symmetry_reproduction\\.venv\\Lib\\site-packages\\laplace\\baselaplace.py:435: UserWarning: By default `link_approx` is `probit`. Make sure to set it equals to the way you want to call `la(test_data, pred_type=..., link_approx=...)`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Apply Laplace approximation\n",
    "laplace_model = Laplace(model, likelihood='classification', subset_of_weights='last_layer', hessian_structure='full')\n",
    "laplace_model.fit(datamodule.train_dataloader())  # Fit the Laplace model on training data\n",
    "laplace_model.optimize_prior_precision()  # Optimize prior precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:01<00:00, 88.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">      Classification       </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     Acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          81.76%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    Brier     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.27002          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   Entropy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.58099          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     NLL      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.57028          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Calibration        </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     ECE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.02009          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     aECE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.02005          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\"> Selective Classification  </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    AUGRC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           4.41%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     AURC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           5.96%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Cov@5Risk   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          52.26%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Risk@80Cov  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          10.91%           </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Classification      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    Acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         81.76%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   Brier    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.27002         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  Entropy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.58099         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    NLL     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.57028         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Calibration       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    ECE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.02009         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    aECE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.02005         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSelective Classification \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   AUGRC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          4.41%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    AURC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          5.96%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Cov@5Risk  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         52.26%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Risk@80Cov \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         10.91%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace Test Results: [{'test/cal/ECE': 0.020094318315386772, 'test/cal/aECE': 0.02005278505384922, 'test/cls/Acc': 0.8176000118255615, 'test/cls/Brier': 0.2700233459472656, 'test/cls/NLL': 0.5702756643295288, 'test/sc/AUGRC': 0.04408245161175728, 'test/sc/AURC': 0.05958322063088417, 'test/sc/Cov@5Risk': 0.5226000547409058, 'test/sc/Risk@80Cov': 0.10912500321865082, 'test/cls/Entropy': 0.5809915661811829}]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "results = trainer.test(model=routine, datamodule=datamodule)\n",
    "print(\"Laplace Test Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGHMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pSGLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_ensemble(model_class, datamodule, num_ensembles=10, save_dir=\"models/deep_ensemble\"):\n",
    "    \"\"\"\n",
    "    Train a deep ensemble of models.\n",
    "    Args:\n",
    "        model_class: The model class (e.g., OptuNet).\n",
    "        datamodule: Data module providing train/val/test splits.\n",
    "        num_ensembles (int): Number of models in the ensemble.\n",
    "        save_dir (str): Directory to save the trained models.\n",
    "    Returns:\n",
    "        list: Trained models.\n",
    "    \"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    trained_models = []\n",
    "    for i in range(num_ensembles):\n",
    "        print(f\"Training model {i + 1}/{num_ensembles}...\")\n",
    "        \n",
    "        # Initialize a new model\n",
    "        model = model_class()\n",
    "        \n",
    "        # Define loss and optimizer\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        routine = ClassificationRoutine(\n",
    "            model=model,\n",
    "            num_classes=datamodule.num_classes,\n",
    "            loss=loss_fn,\n",
    "            optim_recipe=optim_optunet(model),  # Replace with appropriate optimizer\n",
    "            is_ensemble=False\n",
    "        )\n",
    "\n",
    "        # Trainer\n",
    "        trainer = TUTrainer(\n",
    "            accelerator=\"gpu\",\n",
    "            max_epochs=EPOCHS,\n",
    "            enable_progress_bar=True\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.fit(model=routine, datamodule=datamodule)\n",
    "\n",
    "        # Save the model\n",
    "        model_path = save_path / f\"model_{i+1}.pt\"\n",
    "        trainer.save_checkpoint(model_path)\n",
    "\n",
    "        trained_models.append(model)\n",
    "\n",
    "    return trained_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, dataloader):\n",
    "    \"\"\"\n",
    "    Perform inference using a deep ensemble.\n",
    "    Args:\n",
    "        models (list): List of trained models.\n",
    "        dataloader (DataLoader): DataLoader for test data.\n",
    "    Returns:\n",
    "        np.ndarray: Averaged predictions from the ensemble.\n",
    "    \"\"\"\n",
    "    all_preds = []\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        preds = []\n",
    "\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)  # Logits shape: (batch_size, num_classes)\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "        \n",
    "        all_preds.append(np.concatenate(preds, axis=0))  # Combine batches\n",
    "\n",
    "    # Stack predictions from all models and average\n",
    "    ensemble_preds = np.mean(np.stack(all_preds, axis=0), axis=0)\n",
    "    return ensemble_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_models = train_deep_ensemble(OptuNet, datamodule, num_ensembles=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_confidence_interval(model, dataloader, confidence=0.95):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    lower_bounds = []\n",
    "    upper_bounds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "\n",
    "            # For Monte carlo estimation of the ELBO using 3 samples\n",
    "            preds = torch.stack([model(inputs) for _ in range(MC_SAMPLES)], dim=0)\n",
    "\n",
    "            # Calculate mean and standard deviation\n",
    "            preds_mean = preds.mean(dim=0)\n",
    "            preds_std = preds.std(dim=0)\n",
    "\n",
    "            # Apply softmax to the mean predictions for probabilities\n",
    "            preds_mean = F.softmax(preds_mean, dim=1)\n",
    "\n",
    "\n",
    "            # Compute confidence intervals\n",
    "            z_value = st.norm.ppf(1 - (1 - confidence) / 2)  #dynamically computing the z-score for a given confidence level (e.g., 95%, 99%).\n",
    "            ci_lower = preds_mean - z_value * preds_std\n",
    "            ci_upper = preds_mean + z_value * preds_std\n",
    "\n",
    "            all_preds.append(preds_mean)\n",
    "            all_targets.append(targets)\n",
    "            lower_bounds.append(ci_lower)\n",
    "            upper_bounds.append(ci_upper)\n",
    "\n",
    "    # Concatenate results for all batches\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    lower_bounds = torch.cat(lower_bounds)\n",
    "    upper_bounds = torch.cat(upper_bounds)\n",
    "\n",
    "    return all_preds, all_targets, lower_bounds, upper_bounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_sampling(model, data_loader, num_samples=50):\n",
    "    \"\"\"\n",
    "    Perform Monte Carlo Dropout sampling on the model.\n",
    "    Args:\n",
    "        model (nn.Module): Trained OptuDrop model with dropout.\n",
    "        data_loader (DataLoader): DataLoader for test data.\n",
    "        num_samples (int): Number of MC samples.\n",
    "    Returns:\n",
    "        np.ndarray: Array of predictions from all samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        sampled_preds = []\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                sampled_preds.append(outputs.cpu().numpy())\n",
    "        predictions.append(np.concatenate(sampled_preds, axis=0))\n",
    "\n",
    "    return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_weights(models):\n",
    "    \"\"\"\n",
    "    Extract model weights to use as samples for posterior comparison.\n",
    "    Args:\n",
    "        models (list): List of trained models.\n",
    "    Returns:\n",
    "        np.ndarray: Flattened weight arrays for each model.\n",
    "    \"\"\"\n",
    "    weight_samples = []\n",
    "    for model in models:\n",
    "        weights = []\n",
    "        for param in model.parameters():\n",
    "            weights.append(param.detach().cpu().numpy().flatten())\n",
    "        weight_samples.append(np.concatenate(weights))\n",
    "    return np.array(weight_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_model_predictions(models, data_loader):\n",
    "    \"\"\"\n",
    "    Generate predictions from trained models to form the target posterior in prediction space.\n",
    "    Args:\n",
    "        models (list): List of trained models.\n",
    "        data_loader (DataLoader): DataLoader for test data.\n",
    "    Returns:\n",
    "        np.ndarray: Array of predictions for all models.\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)  # Logits\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "        all_predictions.append(np.concatenate(preds, axis=0))  # Combine batches\n",
    "\n",
    "    return np.array(all_predictions)  # Shape: (num_models, num_datapoints, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample model posterior\n",
    "dropout_samples = monte_carlo_sampling(model, datamodule.test_dataloader()[0], num_samples=5)\n",
    "dropout_samples_flat = dropout_samples.reshape(dropout_samples.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target posterior\n",
    "target_predictions = target_model_predictions(posterior_models, datamodule.test_dataloader()[0])\n",
    "# target_posterior_samples = extract_model_weights(posterior_models)\n",
    "target_posterior_samples = target_predictions.reshape(30, -1)  # Shape: (30, 10000 * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMD Score: [0.84391623 0.79542857 0.73696664 0.67138301 0.60202212 0.53211281\n",
      " 0.4643618  0.40076836 0.34261158 0.29054258 0.0176627  0.58616206\n",
      " 0.88334639 0.92725724 0.93261007 0.93324742 0.93332312 0.93326705\n",
      " 0.64932177 0.11887361]\n"
     ]
    }
   ],
   "source": [
    "# Use mmdagg function\n",
    "mmd_score = mmdagg(\n",
    "    X=dropout_samples_flat,\n",
    "    Y=target_posterior_samples,\n",
    "    alpha=0.05,\n",
    "    kernel=\"laplace_gaussian\",\n",
    "    number_bandwidths=10,\n",
    "    weights_type=\"uniform\",\n",
    "    B1=2000,\n",
    "    B2=2000,\n",
    "    B3=50,\n",
    "    seed=42424242,\n",
    ")\n",
    "print(\"MMD Score:\", mmd_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aupr_score(predictions, labels):\n",
    "    n_classes = predictions.shape[1]\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    aupr = {}\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        # Binarize the labels for class i\n",
    "        binary_labels = (labels == i).astype(int)\n",
    "        precision[i], recall[i], _ = precision_recall_curve(binary_labels, predictions[:, i])\n",
    "        aupr[i] = auc(recall[i], precision[i])\n",
    "\n",
    "    # Optional: Aggregate AUPR\n",
    "    mean_aupr = np.mean(list(aupr.values()))\n",
    "    return mean_aupr, aupr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPR95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpr95_score(predictions, labels):\n",
    "    # Calculate predicted classes and confidences\n",
    "    predicted_classes = np.argmax(predictions, axis=1)  # Class with highest probability\n",
    "    confidences = np.max(predictions, axis=1)           # Confidence scores (max probability)\n",
    "    binary_labels = (predicted_classes == labels).astype(int) # Determine binary labels (1 for correct, 0 for incorrect)\n",
    "    fpr, tpr, thresholds = roc_curve(binary_labels, confidences)\n",
    "\n",
    "    # Find the threshold where TPR is closest to 95%\n",
    "    idx = np.where(tpr >= 0.95)[0][0]\n",
    "    return fpr[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ace(preds, targets, num_bins=10):\n",
    "    # Convert preds to probabilities\n",
    "    probs = torch.softmax(preds, dim=1).cpu().numpy()  # For multi-class, probs[:, 1] for class 1, or probs[:, positive_class] for specific class\n",
    "    true_targets = targets.cpu().numpy()\n",
    "    positive_class = 1\n",
    "\n",
    "    # For binary classification, you can use probs[:, 1] for class 1 (positive_class)\n",
    "    prob_true = probs[:, positive_class]\n",
    "\n",
    "    binary_targets = (true_targets == positive_class).astype(int)\n",
    "\n",
    "    # Get calibration curve: This will return the true fraction of positives and predicted probabilities for each bin\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(binary_targets, prob_true, n_bins=num_bins)\n",
    "\n",
    "    # Calculate ACE: This is the average absolute difference between the true fraction of positives and the predicted probability\n",
    "    ace = np.mean(np.abs(fraction_of_positives - mean_predicted_value))\n",
    "    return ace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, test_loader):\n",
    "    model.to(DEVICE)\n",
    "    preds, targets, lower_bounds, upper_bounds = evaluate_confidence_interval(model, test_loader)\n",
    "    predictions = preds.cpu().numpy()\n",
    "    labels = targets.cpu().numpy()\n",
    "\n",
    "    # Scores\n",
    "    mean_aupr, aupr = aupr_score(predictions, labels)\n",
    "    fpr95 = fpr95_score(predictions, labels)\n",
    "    ace = calculate_ace(preds, targets)\n",
    "\n",
    "    print(f\"Mean AUPR: {mean_aupr}\")\n",
    "    print(f\"FPR95: {fpr95}\")\n",
    "    print(f\"ACE: {ace:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUPR: 0.9040881282443637\n",
      "FPR95: 0.6742108397855867\n",
      "ACE: 0.4506\n"
     ]
    }
   ],
   "source": [
    "score_model(model, datamodule.test_dataloader()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
