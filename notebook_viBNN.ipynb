{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtKOd5ZhMT5f"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3837,
     "status": "ok",
     "timestamp": 1734294149291,
     "user": {
      "displayName": "Harriet Manninger",
      "userId": "10267878382098681715"
     },
     "user_tz": -60
    },
    "id": "L0hO7BlBMbw-",
    "outputId": "832909a2-5fea-4476-fac9-03293995a20b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_uncertainty in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from torch_uncertainty) (1.0.12)\n",
      "Requirement already satisfied: lightning>=2.0 in /usr/local/lib/python3.10/dist-packages (from lightning[pytorch-extra]>=2.0->torch_uncertainty) (2.4.0)\n",
      "Requirement already satisfied: torchvision>=0.16 in /usr/local/lib/python3.10/dist-packages (from torch_uncertainty) (0.20.1+cu121)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from torch_uncertainty) (0.8.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from torch_uncertainty) (3.8.0)\n",
      "Requirement already satisfied: rich>=10.2.2 in /usr/local/lib/python3.10/dist-packages (from torch_uncertainty) (13.9.4)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from torch_uncertainty) (0.13.2)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (2024.10.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (0.11.9)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (24.2)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (1.6.0)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (4.12.2)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (2.4.0)\n",
      "Requirement already satisfied: bitsandbytes<1.0,>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from lightning[pytorch-extra]>=2.0->torch_uncertainty) (0.45.0)\n",
      "Requirement already satisfied: hydra-core<2.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from lightning[pytorch-extra]>=2.0->torch_uncertainty) (1.3.2)\n",
      "Requirement already satisfied: jsonargparse<5.0,>=4.27.7 in /usr/local/lib/python3.10/dist-packages (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]>=2.0->torch_uncertainty) (4.34.1)\n",
      "Requirement already satisfied: omegaconf<3.0,>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from lightning[pytorch-extra]>=2.0->torch_uncertainty) (2.3.0)\n",
      "Requirement already satisfied: tensorboardX<3.0,>=2.2 in /usr/local/lib/python3.10/dist-packages (from lightning[pytorch-extra]>=2.0->torch_uncertainty) (2.6.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_uncertainty) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_uncertainty) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_uncertainty) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_uncertainty) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_uncertainty) (1.26.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_uncertainty) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_uncertainty) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_uncertainty) (2.8.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.2.2->torch_uncertainty) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.2.2->torch_uncertainty) (2.18.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (3.16.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (1.3.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn->torch_uncertainty) (2.2.2)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->torch_uncertainty) (0.26.5)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->torch_uncertainty) (0.4.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (3.11.10)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core<2.0,>=1.2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (4.9.3)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]>=2.0->torch_uncertainty) (0.16)\n",
      "Requirement already satisfied: typeshed-client>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]>=2.0->torch_uncertainty) (2.7.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (75.1.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.2.2->torch_uncertainty) (0.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn->torch_uncertainty) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn->torch_uncertainty) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->torch_uncertainty) (1.17.0)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX<3.0,>=2.2->lightning[pytorch-extra]>=2.0->torch_uncertainty) (4.25.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->torch_uncertainty) (2.32.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (1.18.3)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]>=2.0->torch_uncertainty) (6.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning>=2.0->lightning[pytorch-extra]>=2.0->torch_uncertainty) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->torch_uncertainty) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->torch_uncertainty) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->torch_uncertainty) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->torch_uncertainty) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10362,
     "status": "ok",
     "timestamp": 1734294164599,
     "user": {
      "displayName": "Harriet Manninger",
      "userId": "10267878382098681715"
     },
     "user_tz": -60
    },
    "id": "P4jRNL27qVYq",
    "outputId": "218abc6b-abee-478f-f7f2-561b2d5aab61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: blitz-bayesian-pytorch in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from blitz-bayesian-pytorch) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from blitz-bayesian-pytorch) (0.20.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from blitz-bayesian-pytorch) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from blitz-bayesian-pytorch) (1.5.2)\n",
      "Requirement already satisfied: pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from blitz-bayesian-pytorch) (11.0.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->blitz-bayesian-pytorch) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->blitz-bayesian-pytorch) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->blitz-bayesian-pytorch) (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->blitz-bayesian-pytorch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->blitz-bayesian-pytorch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->blitz-bayesian-pytorch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->blitz-bayesian-pytorch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->blitz-bayesian-pytorch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->blitz-bayesian-pytorch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7.0->blitz-bayesian-pytorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->blitz-bayesian-pytorch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install blitz-bayesian-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6357,
     "status": "ok",
     "timestamp": 1734296663414,
     "user": {
      "displayName": "Harriet Manninger",
      "userId": "10267878382098681715"
     },
     "user_tz": -60
    },
    "id": "HYDYGVBDMT5h",
    "outputId": "fb5fb186-9b76-4241-ad7e-62f8b170742a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from torch_uncertainty import TUTrainer\n",
    "from torch_uncertainty.datamodules import MNISTDataModule\n",
    "from torch_uncertainty.losses import ELBOLoss\n",
    "from torch_uncertainty.models.lenet import bayesian_lenet\n",
    "from torch_uncertainty.models import mc_dropout\n",
    "from torch_uncertainty.routines import ClassificationRoutine\n",
    "\n",
    "from blitz.modules import BayesianLinear\n",
    "from blitz.utils import variational_estimator\n",
    "import scipy.stats as st\n",
    "\n",
    "from pathlib import Path\n",
    "from safetensors.torch import load_file\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from google.colab import drive\n",
    "drive.flush_and_unmount()  # Unmount Google Drive\n",
    "drive.mount('/content/drive')  # Remount Google Drive\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YR4kYKQxMT5j"
   },
   "source": [
    "# OptuNet Posterior Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YotA4Fz9MT5k"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_PATH = \"data\"\n",
    "\n",
    "# Parameters from paper\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.04\n",
    "#WEIGHT_DECAY = 2e-4\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "## OptuNet params\n",
    "DROPOUT_RATE = 0.2 # last layer dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDOfBEOUMT5k"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bt64ScFBMT5l"
   },
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "root = Path(DATA_PATH)\n",
    "datamodule = MNISTDataModule(root=root, batch_size=BATCH_SIZE, eval_ood=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBHRG-HBMT5l"
   },
   "source": [
    "## OptuNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziMg5hDRMT5m"
   },
   "outputs": [],
   "source": [
    "# the variational_estimator decorator adjusts the model to compute and optimize\n",
    "# the Evidence Lower Bound (ELBO)\n",
    "@variational_estimator\n",
    "class OptuNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Add layers for OptuNet (use Section C.2.1 from the paper for details)\n",
    "        # Layers: Conv2D (out_ch=2, ks=4, groups=1) -> Max Pooling (ks=3, stride=3) -> ReLU -> Conv2D (out_ch=10, ks=5, groups=2) -> Average Pooling -> ReLU -> Linear 10x10\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=4, groups=1, bias=False)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=10, kernel_size=5, groups=2, bias=False)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=10)\n",
    "        #self.fc1 = nn.Linear(in_features=10 * 2 * 2, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.pool1(self.conv1(x)))  # First conv, max pooling, ReLU\n",
    "        x = self.relu(self.pool2(self.conv2(x)))  # Second conv, avg pooling, ReLU\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        x = self.fc1(x)  # Linear layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyketucyMT5n"
   },
   "source": [
    "## Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87tzD8ymTFOR"
   },
   "outputs": [],
   "source": [
    "class CustomClassificationRoutine(ClassificationRoutine):\n",
    "    def __init__(self, lr_scheduler, num_samples, kl_weight, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        #self.epoch_outputs = []  # Store outputs here if needed\n",
    "        self.num_samples = num_samples\n",
    "        self.kl_weight = kl_weight\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        # Calculate ELBO using sample_elbo\n",
    "        elbo = self.model.sample_elbo(\n",
    "            inputs=inputs,\n",
    "            labels=targets,\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            sample_nbr=self.num_samples,\n",
    "            complexity_cost_weight=self.kl_weight\n",
    "        )\n",
    "\n",
    "        self.log(\"train_elbo\", elbo)\n",
    "        return elbo\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # Step the scheduler if it exists\n",
    "        if self.lr_scheduler:\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "        # Optionally, process self.epoch_outputs here\n",
    "        #self.epoch_outputs.clear()  # Clear outputs for the next epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3386366,
     "status": "ok",
     "timestamp": 1734300720026,
     "user": {
      "displayName": "Harriet Manninger",
      "userId": "10267878382098681715"
     },
     "user_tz": -60
    },
    "id": "sNI_9ELMMT5n",
    "outputId": "352d1a60-20d0-4fc8-8b81-ce0dc1c74f53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: GPU available: False, used: False\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: \n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model            | OptuNet          | 392    | train\n",
      "1 | format_batch_fn  | Identity         | 0      | train\n",
      "2 | val_cls_metrics  | MetricCollection | 0      | train\n",
      "3 | test_cls_metrics | MetricCollection | 0      | train\n",
      "4 | test_id_entropy  | Entropy          | 0      | train\n",
      "5 | mixup            | Identity         | 0      | train\n",
      "--------------------------------------------------------------\n",
      "392       Trainable params\n",
      "0         Non-trainable params\n",
      "392       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "30        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model            | OptuNet          | 392    | train\n",
      "1 | format_batch_fn  | Identity         | 0      | train\n",
      "2 | val_cls_metrics  | MetricCollection | 0      | train\n",
      "3 | test_cls_metrics | MetricCollection | 0      | train\n",
      "4 | test_id_entropy  | Entropy          | 0      | train\n",
      "5 | mixup            | Identity         | 0      | train\n",
      "--------------------------------------------------------------\n",
      "392       Trainable params\n",
      "0         Non-trainable params\n",
      "392       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "30        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=60` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">      Classification       </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     Acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          71.28%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    Brier     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.40296          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   Entropy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.96386          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     NLL      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.84759          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Calibration        </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     ECE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.06973          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     aECE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.06973          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\"> Selective Classification  </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    AUGRC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           8.21%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     AURC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          11.19%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Cov@5Risk   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           nan%            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Risk@80Cov  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          21.15%           </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Classification      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    Acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         71.28%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   Brier    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.40296         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  Entropy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.96386         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    NLL     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.84759         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Calibration       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    ECE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.06973         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    aECE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.06973         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSelective Classification \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   AUGRC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          8.21%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    AURC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         11.19%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Cov@5Risk  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          nan%           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Risk@80Cov \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         21.15%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_ saved to /content/drive/MyDrive/optunet_trained_model_cosine_dec.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def optim_lenet(model: nn.Module):\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=0.04\n",
    "    )\n",
    "    return optimizer\n",
    "\n",
    "# learning rate scheduler to  decay\n",
    "#the learning rate twice during training, at epochs 15 and 30, dividing the learning rate by 2.\n",
    "\"\"\"\n",
    "def scheduler_lenet(optimizer):\n",
    "    scheduler = MultiStepLR(\n",
    "        optimizer,\n",
    "        milestones=[15, 30],  # Epochs at which to decay the learning rate\n",
    "        gamma=0.5,            # Factor by which to multiply the learning rate\n",
    "    )\n",
    "    return scheduler\n",
    "\"\"\"\n",
    "def warmup_cosine_scheduler(optimizer, warmup_steps, total_steps, min_lr=0, max_lr=0.04):\n",
    "    # Define the learning rate scheduler as a Lambda function\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_steps:\n",
    "            # Linear warmup: Increase from 0 to max_lr\n",
    "            return float(epoch) / float(max(1, warmup_steps))\n",
    "        else:\n",
    "            # Cosine decay after warmup\n",
    "            progress = (epoch - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "            return min_lr + 0.5 * (max_lr - min_lr) * (1 + torch.cos(torch.tensor(torch.pi * progress)))\n",
    "\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    return scheduler\n",
    "\n",
    "#trainer = TUTrainer(accelerator=\"gpu\", enable_progress_bar=False, max_epochs=60)\n",
    "trainer = TUTrainer(accelerator=\"cpu\", enable_progress_bar=False, max_epochs=60)\n",
    "\n",
    "# model\n",
    "#model = load_optunet_model(version=1000)\n",
    "model = OptuNet(num_classes=datamodule.num_classes)\n",
    "\n",
    "optimizer = optim_lenet(model)\n",
    "#scheduler = scheduler_lenet(optimizer)\n",
    "# Warmup and cosine decay setup\n",
    "warmup_steps = 5\n",
    "total_steps = 60\n",
    "scheduler = warmup_cosine_scheduler(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "routine = CustomClassificationRoutine(\n",
    "    model=model,\n",
    "    num_classes=datamodule.num_classes,\n",
    "    loss=None, # computed by sample_elbo in training_step()\n",
    "    optim_recipe=optimizer,\n",
    "    lr_scheduler=scheduler,\n",
    "    num_samples = 3,\n",
    "    kl_weight=1/100000\n",
    "    #is_ensemble=True\n",
    ")\n",
    "\n",
    "trainer.fit(model=routine, datamodule=datamodule)\n",
    "results = trainer.test(model=routine, datamodule=datamodule)\n",
    "\n",
    "#save state dictionary on drive (model parameters)\n",
    "model_path = \"/content/drive/MyDrive/optunet_trained_model_cosine_dec.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model_ saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhYmLJaHxBcL"
   },
   "source": [
    "## Confidence Interval Evaluation Function\n",
    "* Sample predictions from your Bayesian model (OptuNet) 3 times for each input.\n",
    "* Calculate the mean and standard deviation of these predictions.\n",
    "* Use these statistics to construct a confidence interval, assuming a Gaussian distribution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mL-Uidnf1Lfw"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_confidence_interval(model, dataloader, confidence=0.95):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    lower_bounds = []\n",
    "    upper_bounds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "\n",
    "            # For Monte carlo estimation of the ELBO using 3 samples\n",
    "            preds = torch.stack([model(inputs) for _ in range(3)], dim=0)\n",
    "\n",
    "            # Calculate mean and standard deviation\n",
    "            preds_mean = preds.mean(dim=0)\n",
    "            preds_std = preds.std(dim=0)\n",
    "\n",
    "            # Apply softmax to the mean predictions for probabilities\n",
    "            preds_mean = F.softmax(preds_mean, dim=1)\n",
    "\n",
    "\n",
    "            # Compute confidence intervals\n",
    "            z_value = st.norm.ppf(1 - (1 - confidence) / 2)  #dynamically computing the z-score for a given confidence level (e.g., 95%, 99%).\n",
    "            ci_lower = preds_mean - z_value * preds_std\n",
    "            ci_upper = preds_mean + z_value * preds_std\n",
    "\n",
    "            all_preds.append(preds_mean)\n",
    "            all_targets.append(targets)\n",
    "            lower_bounds.append(ci_lower)\n",
    "            upper_bounds.append(ci_upper)\n",
    "\n",
    "    # Concatenate results for all batches\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    lower_bounds = torch.cat(lower_bounds)\n",
    "    upper_bounds = torch.cat(upper_bounds)\n",
    "\n",
    "    return all_preds, all_targets, lower_bounds, upper_bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7_jgICHukeZ"
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_PATH = \"data\"\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1734300909850,
     "user": {
      "displayName": "Harriet Manninger",
      "userId": "10267878382098681715"
     },
     "user_tz": -60
    },
    "id": "56ayWkIYurcE",
    "outputId": "1e59ad25-5949-4fd4-a3fc-01a7c107b41f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download dataset to the specified path\n",
    "MNIST(root=DATA_PATH, train=True, download=True)\n",
    "MNIST(root=DATA_PATH, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PO21T8csvOwE"
   },
   "outputs": [],
   "source": [
    "datamodule = MNISTDataModule(\n",
    "    root=Path(DATA_PATH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    eval_ood=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgYc6mrSzhO6"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qGYeQ3cylCX"
   },
   "outputs": [],
   "source": [
    "datamodule.setup(stage='test')\n",
    "test_dataset = datamodule.test  # Ensure `self.test` exists and is initialized correctly.\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9BGYMOesTBz"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.calibration import calibration_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlBl4lmPTpsW"
   },
   "outputs": [],
   "source": [
    "def aupr_score(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()  # Convert to numpy\n",
    "    labels = labels.cpu().numpy() # Convert to numpy\n",
    "\n",
    "    n_classes = predictions.shape[1]\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    aupr = {}\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        # Binarize the labels for class i\n",
    "        binary_labels = (labels == i).astype(int)\n",
    "        precision[i], recall[i], _ = precision_recall_curve(binary_labels, predictions[:, i])\n",
    "        aupr[i] = auc(recall[i], precision[i])\n",
    "\n",
    "    # Optional: Aggregate AUPR\n",
    "    mean_aupr = np.mean(list(aupr.values()))\n",
    "    print(f\"Mean AUPR: {mean_aupr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_9GgJsD-NrF"
   },
   "outputs": [],
   "source": [
    "def fpr95_score(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()  # Convert to numpy\n",
    "    labels = labels.cpu().numpy() # Convert to numpy\n",
    "\n",
    "    # Calculate predicted classes and confidences\n",
    "    predicted_classes = np.argmax(predictions, axis=1)  # Class with highest probability\n",
    "    confidences = np.max(predictions, axis=1)           # Confidence scores (max probability)\n",
    "    binary_labels = (predicted_classes == labels).astype(int) # Determine binary labels (1 for correct, 0 for incorrect)\n",
    "    fpr, tpr, thresholds = roc_curve(binary_labels, confidences)\n",
    "\n",
    "    # Find the threshold where TPR is closest to 95%\n",
    "    idx = np.where(tpr >= 0.95)[0][0]\n",
    "    #return fpr[idx]\n",
    "    print(f\"FPR95: {fpr[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0Qy34Ri-Q96"
   },
   "outputs": [],
   "source": [
    "def ace_score(predictions, labels, n_bins=10):\n",
    "    # Convert predictions and labels to numpy arrays\n",
    "    predicted_probs = predictions.cpu().numpy()\n",
    "    true_labels = labels.cpu().numpy()\n",
    "\n",
    "    # One-hot encode true labels for multi-class calibration\n",
    "    num_classes = predicted_probs.shape[1]\n",
    "    true_labels_one_hot = np.eye(num_classes)[true_labels]  # Shape: (num_samples, num_classes)\n",
    "\n",
    "    # Initialize ACE\n",
    "    ace = 0.0\n",
    "\n",
    "    # Loop over each class\n",
    "    for class_idx in range(num_classes):\n",
    "        # Get predicted probabilities and true labels for the current class\n",
    "        prob_pred = predicted_probs[:, class_idx]\n",
    "        prob_true = true_labels_one_hot[:, class_idx]\n",
    "\n",
    "        # Compute calibration curve\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(prob_true, prob_pred, n_bins=n_bins)\n",
    "\n",
    "        # Compute ACE for this class\n",
    "        ace += np.mean(np.abs(fraction_of_positives - mean_predicted_value))\n",
    "\n",
    "    # Average over all classes\n",
    "    ace /= num_classes\n",
    "    print(f\"ACE score: {ace}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNfPeifkxwMm"
   },
   "outputs": [],
   "source": [
    "def monte_carlo_sampling(model, data_loader, num_samples=100):\n",
    "    # model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        sampled_preds = []\n",
    "        for inputs, _ in data_loader:\n",
    "            # inputs = inputs.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                sampled_preds.append(outputs.cpu().numpy())\n",
    "        predictions.append(np.concatenate(sampled_preds, axis=0))\n",
    "\n",
    "    return np.array(predictions) # Shape: (num_samples, num_examples, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eb01el-CxMox"
   },
   "outputs": [],
   "source": [
    "def target_model_predictions(models, data_loader):\n",
    "    all_predictions = []\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)  # Logits\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "        all_predictions.append(np.concatenate(preds, axis=0))  # Combine batches\n",
    "\n",
    "    return np.array(all_predictions)  # Shape: (num_models, num_datapoints, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqMfRRrbxdPA"
   },
   "outputs": [],
   "source": [
    "def calculate_mmd(model, posterior_models, test_dataset, num_samples=100):\n",
    "    # # Posterior estimation with weights?\n",
    "    # target_weights = generate_target_samples(posterior_models)\n",
    "    # source_weights = generate_source_samples(model, test_dataset, num_samples=num_samples)\n",
    "\n",
    "    # Posterior estimation with predictions?\n",
    "    target_preds = target_model_predictions(posterior_models, test_dataset)\n",
    "    source_preds = monte_carlo_sampling(model, test_dataset, num_samples=num_samples)\n",
    "    target_avg = np.mean(target_preds, axis=1)\n",
    "    source_avg = np.mean(source_preds, axis=1)\n",
    "\n",
    "    mmd_preds = mmdagg(\n",
    "        X=source_avg,\n",
    "        Y=target_avg,\n",
    "        alpha=0.05,\n",
    "        kernel=\"laplace_gaussian\",\n",
    "        number_bandwidths=10,\n",
    "        weights_type=\"uniform\",\n",
    "        B1=2000,\n",
    "        B2=2000,\n",
    "        B3=50,\n",
    "        seed=42424242\n",
    "    )\n",
    "\n",
    "    return None, mmd_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4354,
     "status": "ok",
     "timestamp": 1734300940443,
     "user": {
      "displayName": "Harriet Manninger",
      "userId": "10267878382098681715"
     },
     "user_tz": -60
    },
    "id": "cPffa_yorhAl",
    "outputId": "dc7e712c-dc24-44e7-a3cc-55fde05c31fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-ff69aff6c657>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUPR: 0.7674309046280484\n",
      "FPR95: 0.7889972144846796\n",
      "ACE score: 0.09389052589950675\n"
     ]
    }
   ],
   "source": [
    "#(reinitializing the optunet architecture)\n",
    "model = OptuNet(num_classes=datamodule.num_classes)\n",
    "#load state dictionary\n",
    "model_path = \"/content/drive/MyDrive/optunet_trained_model_cosine_dec.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(DEVICE)  # Send model to appropriate device\n",
    "print(f\"Model loaded successfully!\")\n",
    "preds, targets, lower_bounds, upper_bounds = evaluate_confidence_interval(model, test_dataloader, confidence=0.95)\n",
    "aupr_score(preds, targets)\n",
    "fpr95_score(preds, targets)\n",
    "ace_score(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFUnv2n1t7Ue"
   },
   "outputs": [],
   "source": [
    "def load_target_models(version: int):\n",
    "    posterior_models = []\n",
    "    num_models = 75\n",
    "    for i in range(num_models):\n",
    "      #Load the model corresponding to the given version.\n",
    "      model = OptuNet(num_classes=datamodule.num_classes)\n",
    "      #path = Path(f\"models/mnist-optunet-0-8191/version_{version}.safetensors\")\n",
    "      #notebook_dir = Path(\"/content/drive/MyDrive/DL,\\ adv/project\")\n",
    "      path = f\"/content/drive/MyDrive/DL, adv/project/our_models/model_{i}.pth\"\n",
    "\n",
    "      print(f\"os.path.exists(path): {os.path.exists(path)}\")\n",
    "\n",
    "      if not os.path.exists(path):\n",
    "          raise ValueError(\"File does not exist\")\n",
    "\n",
    "      state_dict = load_file(path)\n",
    "\n",
    "      model.load_state_dict(state_dict=state_dict)\n",
    "      return posterior_models"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
